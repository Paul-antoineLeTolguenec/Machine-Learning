{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZiULAOAv6AZ"
   },
   "source": [
    "# Machine Learning Programming Exercise 6: <ins>Supervised classification</ins>\n",
    "\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "\n",
    "Nous allons dans ce TP classer automatiquement des patchs extraits d'images sonar (cf. figure ci-dessous) en types de fond marin (roches, sables, vases, rides de sable verticales et à 45°, [Posidonie](https://fr.wikipedia.org/wiki/Posidonia_oceanica)).\n",
    "\n",
    "Quelques exemples de patchs d'image sonar de fond marin:\n",
    "<img src=\"imgs/screenshot001.png\" />\n",
    "\n",
    "\n",
    "L'objectif est d'écrire des scripts permettant de mettre en \\oe uvre un système basé sur différentes approches supervisées de machine learning. Ces scripts devront ainsi suivre la chaîne générale décrite en cours (à l'exception de la phase de captation; cf. figure ci-dessous ) :\n",
    "* prétraitements\n",
    "* extraction des descripteurs\n",
    "* apprentissage d'un modèle de classement\n",
    "* classement des pixels\n",
    "* évaluation du classifieur appris\n",
    "\n",
    "<img src=\"imgs/screenshot002.png\" />\n",
    "\n",
    "Le TP est globalement organisé de la manière suivante\n",
    "* **Données**\n",
    " 1. tout d'abord apprendre les modèles de classement (classifieurs) sur les données brutes (descripteurs=features=valeurs des pixels) \n",
    " 2. puis dans un second temps sur des descripteurs extraits à partir d'un algorithme appelé [scattering operator](https://www.di.ens.fr/data/scattering) (le fonctionnement exact n'est pas au programme mais il s'apparente à une banque de filtres mise en cascade). \n",
    "\n",
    "* **Prétraitements** Aucun prétraitement ne sera réalisé. \n",
    "\n",
    "* **Ensembles de données**\n",
    " 1. Les ensembles de données seront composés de 1/3 de la base totale d'images. \n",
    " 2. Dans un second temps, nous procéderons par [validation croisée](https://scikit-learn.org/stable/modules/cross_validation.html) car la base d'images est de taille réduite.\n",
    "* **Algorithmes** \n",
    "    Concernant les algorithmes supervisés de machine learning, l'objectif est d'utiliser les deux algorithmes de regression logistique et de réseaux de neurones que vous avez développés aux TP précédents et de découvrir le package python [scikit-learn](http://scikit-learn.org/stable/user_guide.html) qui vous permettra d'utiliser les algorithmes de [régression logistique](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), [réseaux de neurones](https://scikit-learn.org/stable/modules/neural_networks_supervised.html), [random forests](https://scikit-learn.org/stable/modules/ensemble.html#forest) et [svm](https://scikit-learn.org/stable/modules/svm.html#svm-classification).\n",
    "\n",
    "* Pour commencer avec cette séance, vous aurez besoin de **télécharger** le _starter code_  disponible sur le lien Moodle du cours.\n",
    "\n",
    "<span style='color:red'>**Dans cet exercice, il vous est demandé de fournir un rapport regroupant les réponses aux questions, vos analyses et vos codes. Ce rapport pourra prendre la forme d'un pdf ou d'un jupyter notebook. Il est de plus conseillé de faire tourner les codes sur google colab si votre machine manque de puissance (dans ce cas un jupyter notebook est nécessaire).**</span>\n",
    "\n",
    "\n",
    "## Fichiers inclus dans le starter code pour cette séance\n",
    "* **pythonTools.py** - fonctions utiles pour l'affichage, le chargement des données et l'évaluation des performances\n",
    "* **usefulCmds.py** - quelques commandes pour faciliter l'import des patchs\n",
    "* **dataSet** - répertoire avec les images et les labels correspondants\n",
    "* **dataSet\\imdb_200x200_SmallSonarTex_db_6classes_scatValOnly.mat** - fichier matlab contenant les descripteurs extraits des images par le scattering operator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Y6TWZBv6Ab"
   },
   "source": [
    "# Part 0: intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsF0iPYhv6Ae"
   },
   "source": [
    "## 0.1 imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9vExjoNv6Af"
   },
   "source": [
    "_Your commented code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "vQhXCVuAv6Ag",
    "outputId": "e52b044d-8b30-4414-9a24-3a1bab9807f0"
   },
   "outputs": [],
   "source": [
    "from pythonTools import *\n",
    "from usefulCmds import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEAis_r1v6Ar"
   },
   "source": [
    "## 0.2 Examen des données\n",
    "\n",
    "Écrire des lignes de code permettant:\n",
    "* de charger les données comprises dans le fichier _labels.csv_,\n",
    "* de mettre en matrice les descripteurs de l'ensemble de la base d'images\n",
    "* d'afficher les images avec la fonction _plot\\_batch()_ du fichier \\_pythonTools.py_,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5dDA5Kiv6Ar"
   },
   "source": [
    "_Your commented code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "xGqVIrCxv6At",
    "outputId": "d4af001f-86e6-4ebf-d9f5-72269c8ec4fe",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------Data information--------------------------------------------\n",
      "Dimensions du dataset: (360, 2)\n",
      "['Posidonia', 'Ripple 45°', 'Rock', 'Sand', 'Silt', 'Ripple vertical']\n",
      "                 id\n",
      "seafloor           \n",
      "Posidonia        60\n",
      "Ripple 45°       60\n",
      "Ripple vertical  60\n",
      "Rock             60\n",
      "Sand             60\n",
      "Silt             60\n",
      "----------------------------------------Batch--------------------------------------------\n",
      "                                 id         seafloor\n",
      "271             Silt_Sure.02510.png             Silt\n",
      "339  Ripple vertical_Sure.00238.png  Ripple vertical\n",
      "257             Silt_Sure.00166.png             Silt\n",
      "125             Rock_Sure.00108.png             Rock\n",
      "211             Sand_Sure.00079.png             Sand\n"
     ]
    }
   ],
   "source": [
    "# Charger le fichier CSV\n",
    "DATASET_PATH = r'./dataset/imgs/'\n",
    "LABEL_PATH = r'./dataset/labels/labels.csv'\n",
    "dataset_df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------Data information--------------------------------------------\")\n",
    "#print nombre de données\n",
    "print(\"Dimensions du dataset:\",dataset_df.shape)\n",
    "\n",
    "#Nombre de classes:\n",
    "print(pd.unique(dataset_df['seafloor']).tolist())\n",
    "#Nombre d'images par classes\n",
    "print(dataset_df.groupby('seafloor').nunique())\n",
    "\n",
    "#Ploting batch\n",
    "print(\"----------------------------------------Batch--------------------------------------------\")\n",
    "#getting 5 random data\n",
    "batch=load_batch(dataset_df,5)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            ./dataset/imgs/Posidonia_Sure.00001.png\n",
      "1            ./dataset/imgs/Posidonia_Sure.00002.png\n",
      "2            ./dataset/imgs/Posidonia_Sure.00008.png\n",
      "3            ./dataset/imgs/Posidonia_Sure.00011.png\n",
      "4            ./dataset/imgs/Posidonia_Sure.00029.png\n",
      "                           ...                      \n",
      "355    ./dataset/imgs/Ripple vertical_Sure.00562.png\n",
      "356    ./dataset/imgs/Ripple vertical_Sure.00564.png\n",
      "357    ./dataset/imgs/Ripple vertical_Sure.00669.png\n",
      "358    ./dataset/imgs/Ripple vertical_Sure.00843.png\n",
      "359    ./dataset/imgs/Ripple vertical_Sure.00855.png\n",
      "Name: image_path, Length: 360, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset_df['image_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbI5IPb4v6Az"
   },
   "source": [
    "**Question: Quels sont le nombre de données et le nombre de descripteurs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ2JIo5Jv6A0"
   },
   "source": [
    "Il y a 360 données. C'est données sont réparties en 6 classes. \n",
    "Il y a 60 images par classes.\n",
    "\n",
    "Il y a 40000 descipteurs par images.(voir plus bas)\n",
    "\n",
    "Posidonia        60         \n",
    "Ripple 45°       60         \n",
    "Ripple vertical  60         \n",
    "Rock             60     \n",
    "Sand             60       \n",
    "Silt             60         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add another column to the labels dataset to identify image path\n",
    "dataset_df['image_path'] = dataset_df.apply( lambda row: (DATASET_PATH + row[\"id\"] ), axis=1)\n",
    "\n",
    "# Chargement des images\n",
    "feature_values = np.array([plt.imread(img).reshape(40000,) for img in dataset_df['image_path'].values.tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCk7ykINv6A1"
   },
   "source": [
    "## 0.3 prétraitements des labels\n",
    "\n",
    "Écrire des lignes de code, un script ou une fonction _preprocessing()_ permettant:\n",
    "* de disposer des labels dans différents [codages](https://scikit-learn.org/stable/modules/preprocessing_targets.html) (noms, indices, one-hot-encoding, etc.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1GzYnP5v6A3"
   },
   "source": [
    "_Your commented code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CW41N-eHv6A3",
    "outputId": "19c1a8ec-d25b-4525-d245-c3b35983b52b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Récupération des labels\n",
    "label_names = dataset_df['seafloor']\n",
    "label_names_unique = label_names.unique()\n",
    "\n",
    "#  transformation des labels selon différents codages\n",
    "# indices\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(label_names_unique)\n",
    "label_indices = le.transform(label_names_unique)\n",
    "\n",
    "# one-hot-encoding\n",
    "label_ohe = pd.get_dummies(label_names.reset_index(drop=True)).values\n",
    "\n",
    "# Getting labels for our dataset\n",
    "y = le.transform(label_names)\n",
    "#y = tf.keras.utils.to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tu9u9Oqv6A8"
   },
   "source": [
    "## 0.4 Séparation des données en ensembles \n",
    "\n",
    "Écrire des lignes de code, un script ou une fonction _preprocessing()_ permettant:\n",
    "* de [normaliser](https://scikit-learn.org/stable/modules/preprocessing.html) les données si besoin \n",
    "* de [créer deux ensembles](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) un pour l'apprentissage et un pour le test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1p9-v4cv6A9"
   },
   "source": [
    "_Your commented code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "xjAfwumev6A-",
    "outputId": "2337cfe2-b29f-4769-d502-7ef858fc7505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 40000) (270,) (90, 40000) (90,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_values, y, test_size = 0.25, random_state = 0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcDIy1Nmv6BD"
   },
   "source": [
    "<strong>Question:</strong> Pour <ins>chaque ensemble de données</ins> et <ins>pour chaque classe</ins>, quels sont le nombre de données et le nombre de descripteurs? Est-ce important? Pourquoi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nCFuRy2v6BE"
   },
   "source": [
    "Il y a 75 pourcents des données dans Xtrain et 25 pourcents des données dans Xtest.\n",
    "\n",
    "X_train= 270 examples\n",
    "\n",
    "X_test= 90 examples\n",
    "\n",
    "Pour chaques images il y a 40000 descripteurs (200*200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1p9-v4cv6A9"
   },
   "source": [
    "_Your commented code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "xjAfwumev6A-",
    "outputId": "2337cfe2-b29f-4769-d502-7ef858fc7505"
   },
   "outputs": [],
   "source": [
    "#Lets create a validation set\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "#print(X_train.shape, y_train.shape, X_test.shape, y_test.shape,X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6nioyvrv6BE"
   },
   "source": [
    "# Part 1 approches supervisées sur données brutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtkGMAtqv6BF"
   },
   "source": [
    "<strong><ins>Question</ins>: Y-a-t-il besoin de normaliser les descripteurs? Si oui, que faut-il conserver comme information et pourquoi?</strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbFeb9jAv6BG"
   },
   "source": [
    "Il est necessaire de normaliser les descripteurs. En effet de cette façon le modèle converge plus rapidement.On associe à chaque descripteurs la même importance dans la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "#X_val = sc.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01PQN1yev6Bl"
   },
   "source": [
    "**<ins>Question</ins>: Nous allons apprendre les modèles suivants:\n",
    "* régression logistique régularisée et réseaux de neurones développés dans les tps précédents,\n",
    "* régression logistique, réseaux de neurones (solver=lbfgs), svm et random forest en utilisant les fonctions du package scikit-learn\n",
    "\n",
    "Faire la liste des hyper-paramètres (paramètre uniquement lié à l'algorithme d'apprentissage) de chaque algorithme. Comment fixe-t-on leurs valeurs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxHG8upGv6BO"
   },
   "source": [
    "# Pour la regression logistique regularisée les hyper-paramètres du modèle sont :\n",
    "\n",
    "-le taux d'apprendtissage\n",
    "\n",
    "-le paramètre de régularisation\n",
    "\n",
    "# Pour le réseau de neurones:\n",
    "\n",
    "-le taux d'apprentissage\n",
    "\n",
    "-le nombre d'hidden layer\n",
    "\n",
    "# Pour le random forest:\n",
    "\n",
    "-la pronfondeur des arbres\n",
    "\n",
    "-le nombre d'arbres dans la forêt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE40sCAMv6BG"
   },
   "source": [
    "<strong><ins>Question</ins>: Fixez au mieux les valeurs des hyperparamètres, réalisez l'apprentissage des modèles suivants: \n",
    "* régression logistique régularisée et réseaux de neurones développés dans les tps précédents,\n",
    "* régression logistique, réseaux de neurones, svm et random forest en utilisant les fonctions du package scikit-learn\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRvvdn-Ev6BH"
   },
   "source": [
    "_Your code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------- \n",
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "(400010,)\n",
      "(66,)\n",
      "(400076,)\n"
     ]
    }
   ],
   "source": [
    "#-------------------REGRESSION LOGISTIQUE REGULARISEE---------------------\n",
    "from linearRegCostFunction import linearRegCostFunction\n",
    "from trainLinearReg import trainLinearReg\n",
    "from learningCurve import learningCurve\n",
    "from polyFeatures import polyFeatures\n",
    "from featureNormalize import featureNormalize\n",
    "from plotFit import plotFit\n",
    "from validationCurve import validationCurve\n",
    "import numpy as np\n",
    "from nnCostFunction import nnCostFunction\n",
    "import scipy.io\n",
    "from scipy.optimize import minimize, fmin_cg, fmin_l_bfgs_b\n",
    "#-------------------NEURAL NETWORK-----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "    E = 0.12\n",
    "    W = np.random.rand(L_out, 1+L_in) * 2 * E - E\n",
    "    return W\n",
    "\n",
    "\n",
    "input_layer_size  = 40000\n",
    "hidden_layer_size = 10 \n",
    "num_labels = 6    \n",
    "\n",
    "\n",
    "\n",
    "print('\\n -------------------------- \\n')\n",
    "print('Initializing Neural Network Parameters ...')\n",
    "\n",
    "\n",
    "initial_theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "\n",
    "print(initial_theta1.T.ravel().shape)\n",
    "print(initial_theta2.T.ravel().shape)\n",
    "\n",
    "initial_nn_params = np.hstack((initial_theta1.T.ravel(), initial_theta2.T.ravel()))\n",
    "\n",
    "\n",
    "print(initial_nn_params.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 1 3 0 4 4 5 3 0 3 4 4 4 1 1 2 4 5 5 3 5 3 4 1 0 3 0 5 5 3 5 5 5 5 5 3\n",
      " 0 0 2 0 3 1 1 4 3 2 4 3 1 0 5 2 1 3 2 5 2 0 5 5 5 2 0 4 2 3 3 3 4 4 4 5 0\n",
      " 4 2 2 4 0 4 3 1 1 3 2 4 0 2 1 4 1 4 2 2 2 3 2 3 1 4 2 1 2 4 2 1 3 3 3 0 4\n",
      " 1 1 2 5 3 2 0 1 5 1 1 2 4 3 4 5 4 2 0 1 0 1 2 0 2 3 0 0 1 4 4 2 5 2 0 2 2\n",
      " 4 2 1 0 0 0 1 5 1 0 0 2 0 0 0 3 0 4 5 5 5 3 4 1 5 3 4 0 0 0 4 2 1 3 5 1 2\n",
      " 1 2 3 1 0 4 5 4 3 3 4 3 2 2 5 4 1 3 2 2 1 1 5 1 5 0 5 2 0 1 5 2 0 0 2 3 5\n",
      " 0 4 3 3 5 4 0 0 3 4 2 5 2 3 5 2 3 1 4 5 1 5 1 2 0 3 2 2 3 2 0 4 2 1 1 1 5\n",
      " 5 5 4 0 4 5 2 4 1 0 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------- \n",
      "\n",
      "Training Neural Network... \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (270,5) (270,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-ca27ef24f0ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgradFunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnnCostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcostFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_nn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradFunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnn_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 198\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    199\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    200\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    306\u001b[0m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[1;32m    307\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mfunc_and_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun_and_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m--> 262\u001b[0;31m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-ca27ef24f0ec>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcostFunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnnCostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgradFunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnnCostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/Machine-Learning/nnCostFunction.py\u001b[0m in \u001b[0;36mnnCostFunction\u001b[0;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0moutputa3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0minner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputa3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputa3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutputa3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (270,5) (270,6) "
     ]
    }
   ],
   "source": [
    "print('\\n -------------------------- \\n')\n",
    "print('Training Neural Network... ')\n",
    "\n",
    "Lambda = 1\n",
    "costFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X_train, y_train, Lambda)[0]\n",
    "gradFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X_train, y_train, Lambda)[1]\n",
    "\n",
    "result = fmin_cg(costFunc, fprime=gradFunc, x0=initial_nn_params, maxiter=50, disp=True,full_output=True)\n",
    "\n",
    "nn_params = result[0]\n",
    "cost = result[1]\n",
    "#theta1 = nn_params[0:(hidden_layer_size*(input_layer_size+1))].reshape((input_layer_size+1),hidden_layer_size).T\n",
    "#theta2 = nn_params[(hidden_layer_size*(input_layer_size+1)):].reshape((hidden_layer_size+1),num_labels).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cb5ac7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cb516f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cb5033268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cb4f2aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cb4e55950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.4333333333333334 {'batch_size': 50, 'epochs': 150, 'optimizer': 'adam'}\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cb4c9c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "#-------------------NEURAL NETWORK WITH SCIKIT LEARN AND TENSORFLOW---------------------\n",
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#Fonction de construction du CNN\n",
    "def build_classifier(optimizer='adam'):\n",
    "    # Initialising the CNN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Step 4 - Full connection\n",
    "    classifier.add(Dense(units = 128, activation = 'relu',input_dim = 40000))\n",
    "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "    #classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "    classifier.add(Dense(units = 6, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the CNN\n",
    "    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "#Instance du classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "\n",
    "#Liste de paramétres à tester lors de l'entraînement.\n",
    "parameters = {'batch_size': [50],\n",
    "              'epochs': [150],\n",
    "              'optimizer': ['adam']}\n",
    "#Création de la grille d'entraînement.\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5)\n",
    "#On entraîne avec les différents paramètres spécipfiés dans la liste.\n",
    "grid_search = grid_search.fit(X_train, y_train,verbose=0)\n",
    "#On évalue le score et les meilleurs paramétres\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(best_accuracy,best_parameters)\n",
    "score = grid_search.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.9770 - accuracy: 0.2000 - val_loss: 1.9352 - val_accuracy: 0.2778\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.7927 - accuracy: 0.3111 - val_loss: 1.8414 - val_accuracy: 0.3556\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7048 - accuracy: 0.3556 - val_loss: 1.8220 - val_accuracy: 0.3778\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.6538 - accuracy: 0.3704 - val_loss: 1.8104 - val_accuracy: 0.3889\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6041 - accuracy: 0.4074 - val_loss: 1.8030 - val_accuracy: 0.3889\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.5546 - accuracy: 0.4296 - val_loss: 1.8055 - val_accuracy: 0.3889\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.5114 - accuracy: 0.4444 - val_loss: 1.8063 - val_accuracy: 0.3889\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1.4690 - accuracy: 0.4481 - val_loss: 1.8036 - val_accuracy: 0.3889\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4272 - accuracy: 0.4556 - val_loss: 1.7963 - val_accuracy: 0.3889\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.3899 - accuracy: 0.4889 - val_loss: 1.7932 - val_accuracy: 0.3889\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.3533 - accuracy: 0.5000 - val_loss: 1.7919 - val_accuracy: 0.4000\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3157 - accuracy: 0.5074 - val_loss: 1.7904 - val_accuracy: 0.4000\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.2802 - accuracy: 0.5222 - val_loss: 1.7890 - val_accuracy: 0.4000\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.2458 - accuracy: 0.5370 - val_loss: 1.7841 - val_accuracy: 0.4000\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2150 - accuracy: 0.5630 - val_loss: 1.7803 - val_accuracy: 0.4000\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.1819 - accuracy: 0.5741 - val_loss: 1.7776 - val_accuracy: 0.4000\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1528 - accuracy: 0.5889 - val_loss: 1.7750 - val_accuracy: 0.3889\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1229 - accuracy: 0.6037 - val_loss: 1.7735 - val_accuracy: 0.3889\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.0949 - accuracy: 0.6296 - val_loss: 1.7779 - val_accuracy: 0.3889\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.0675 - accuracy: 0.6296 - val_loss: 1.7730 - val_accuracy: 0.3889\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.0401 - accuracy: 0.6407 - val_loss: 1.7737 - val_accuracy: 0.3889\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.0131 - accuracy: 0.6556 - val_loss: 1.7686 - val_accuracy: 0.3778\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.9884 - accuracy: 0.6704 - val_loss: 1.7649 - val_accuracy: 0.3778\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9639 - accuracy: 0.6889 - val_loss: 1.7608 - val_accuracy: 0.3778\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9401 - accuracy: 0.6926 - val_loss: 1.7613 - val_accuracy: 0.3778\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.9186 - accuracy: 0.7111 - val_loss: 1.7566 - val_accuracy: 0.3778\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.8941 - accuracy: 0.7222 - val_loss: 1.7560 - val_accuracy: 0.3778\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.8728 - accuracy: 0.7370 - val_loss: 1.7549 - val_accuracy: 0.3889\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.8516 - accuracy: 0.7630 - val_loss: 1.7545 - val_accuracy: 0.3778\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.8317 - accuracy: 0.7852 - val_loss: 1.7546 - val_accuracy: 0.3889\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.8116 - accuracy: 0.7926 - val_loss: 1.7505 - val_accuracy: 0.3778\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7933 - accuracy: 0.8111 - val_loss: 1.7509 - val_accuracy: 0.3889\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7752 - accuracy: 0.8148 - val_loss: 1.7492 - val_accuracy: 0.3889\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7567 - accuracy: 0.8296 - val_loss: 1.7444 - val_accuracy: 0.3889\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.7402 - accuracy: 0.8481 - val_loss: 1.7432 - val_accuracy: 0.3889\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.7248 - accuracy: 0.8519 - val_loss: 1.7423 - val_accuracy: 0.4000\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.7072 - accuracy: 0.8556 - val_loss: 1.7363 - val_accuracy: 0.3889\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.6916 - accuracy: 0.8741 - val_loss: 1.7351 - val_accuracy: 0.3889\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6776 - accuracy: 0.8704 - val_loss: 1.7343 - val_accuracy: 0.4000\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6614 - accuracy: 0.8852 - val_loss: 1.7301 - val_accuracy: 0.3889\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6495 - accuracy: 0.8926 - val_loss: 1.7326 - val_accuracy: 0.4000\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.6354 - accuracy: 0.8963 - val_loss: 1.7360 - val_accuracy: 0.4000\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6215 - accuracy: 0.9037 - val_loss: 1.7352 - val_accuracy: 0.4000\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6087 - accuracy: 0.9074 - val_loss: 1.7315 - val_accuracy: 0.4000\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5963 - accuracy: 0.9111 - val_loss: 1.7279 - val_accuracy: 0.4000\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.5844 - accuracy: 0.9185 - val_loss: 1.7270 - val_accuracy: 0.4000\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5728 - accuracy: 0.9185 - val_loss: 1.7245 - val_accuracy: 0.4000\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5612 - accuracy: 0.9222 - val_loss: 1.7265 - val_accuracy: 0.3889\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5510 - accuracy: 0.9222 - val_loss: 1.7263 - val_accuracy: 0.3889\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5392 - accuracy: 0.9259 - val_loss: 1.7291 - val_accuracy: 0.4000\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5296 - accuracy: 0.9333 - val_loss: 1.7276 - val_accuracy: 0.4000\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.5199 - accuracy: 0.9407 - val_loss: 1.7289 - val_accuracy: 0.4000\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5091 - accuracy: 0.9444 - val_loss: 1.7199 - val_accuracy: 0.4000\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5004 - accuracy: 0.9481 - val_loss: 1.7168 - val_accuracy: 0.4111\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4905 - accuracy: 0.9481 - val_loss: 1.7213 - val_accuracy: 0.4000\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4818 - accuracy: 0.9481 - val_loss: 1.7259 - val_accuracy: 0.4000\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.4735 - accuracy: 0.9481 - val_loss: 1.7273 - val_accuracy: 0.4111\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4653 - accuracy: 0.9519 - val_loss: 1.7278 - val_accuracy: 0.4000\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.4568 - accuracy: 0.9556 - val_loss: 1.7247 - val_accuracy: 0.4222\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.4487 - accuracy: 0.9556 - val_loss: 1.7208 - val_accuracy: 0.4000\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4410 - accuracy: 0.9556 - val_loss: 1.7190 - val_accuracy: 0.4111\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.4331 - accuracy: 0.9556 - val_loss: 1.7192 - val_accuracy: 0.4111\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.4261 - accuracy: 0.9556 - val_loss: 1.7228 - val_accuracy: 0.4222\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4181 - accuracy: 0.9593 - val_loss: 1.7245 - val_accuracy: 0.4222\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.4108 - accuracy: 0.9593 - val_loss: 1.7282 - val_accuracy: 0.4222\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4041 - accuracy: 0.9593 - val_loss: 1.7255 - val_accuracy: 0.4222\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3978 - accuracy: 0.9593 - val_loss: 1.7260 - val_accuracy: 0.4222\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.3909 - accuracy: 0.9593 - val_loss: 1.7294 - val_accuracy: 0.4333\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3847 - accuracy: 0.9593 - val_loss: 1.7229 - val_accuracy: 0.4333\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3777 - accuracy: 0.9667 - val_loss: 1.7171 - val_accuracy: 0.4222\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.3717 - accuracy: 0.9667 - val_loss: 1.7192 - val_accuracy: 0.4111\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3661 - accuracy: 0.9630 - val_loss: 1.7226 - val_accuracy: 0.4333\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.3600 - accuracy: 0.9630 - val_loss: 1.7227 - val_accuracy: 0.4333\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3552 - accuracy: 0.9630 - val_loss: 1.7248 - val_accuracy: 0.4333\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3482 - accuracy: 0.9667 - val_loss: 1.7223 - val_accuracy: 0.4333\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.3437 - accuracy: 0.9704 - val_loss: 1.7224 - val_accuracy: 0.4333\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3381 - accuracy: 0.9704 - val_loss: 1.7231 - val_accuracy: 0.4333\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3331 - accuracy: 0.9704 - val_loss: 1.7275 - val_accuracy: 0.4222\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.3278 - accuracy: 0.9704 - val_loss: 1.7188 - val_accuracy: 0.4444\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.3229 - accuracy: 0.9704 - val_loss: 1.7212 - val_accuracy: 0.4333\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3181 - accuracy: 0.9741 - val_loss: 1.7240 - val_accuracy: 0.4222\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3130 - accuracy: 0.9741 - val_loss: 1.7274 - val_accuracy: 0.4333\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3087 - accuracy: 0.9741 - val_loss: 1.7299 - val_accuracy: 0.4333\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3039 - accuracy: 0.9741 - val_loss: 1.7258 - val_accuracy: 0.4333\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2996 - accuracy: 0.9741 - val_loss: 1.7241 - val_accuracy: 0.4333\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2951 - accuracy: 0.9741 - val_loss: 1.7246 - val_accuracy: 0.4333\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2905 - accuracy: 0.9741 - val_loss: 1.7269 - val_accuracy: 0.4333\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.2861 - accuracy: 0.9741 - val_loss: 1.7311 - val_accuracy: 0.4333\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2827 - accuracy: 0.9704 - val_loss: 1.7296 - val_accuracy: 0.4333\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2779 - accuracy: 0.9778 - val_loss: 1.7303 - val_accuracy: 0.4222\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.2742 - accuracy: 0.9889 - val_loss: 1.7273 - val_accuracy: 0.4333\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2704 - accuracy: 0.9889 - val_loss: 1.7327 - val_accuracy: 0.4333\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2664 - accuracy: 0.9852 - val_loss: 1.7350 - val_accuracy: 0.4222\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2628 - accuracy: 0.9852 - val_loss: 1.7299 - val_accuracy: 0.4222\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.2593 - accuracy: 0.9852 - val_loss: 1.7266 - val_accuracy: 0.4333\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2553 - accuracy: 0.9889 - val_loss: 1.7346 - val_accuracy: 0.4333\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2521 - accuracy: 0.9852 - val_loss: 1.7405 - val_accuracy: 0.4333\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2491 - accuracy: 0.9852 - val_loss: 1.7381 - val_accuracy: 0.4333\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2452 - accuracy: 0.9889 - val_loss: 1.7307 - val_accuracy: 0.4444\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2421 - accuracy: 0.9889 - val_loss: 1.7316 - val_accuracy: 0.4333\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2384 - accuracy: 0.9852 - val_loss: 1.7322 - val_accuracy: 0.4333\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2358 - accuracy: 0.9889 - val_loss: 1.7308 - val_accuracy: 0.4333\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2325 - accuracy: 0.9889 - val_loss: 1.7309 - val_accuracy: 0.4333\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2290 - accuracy: 0.9889 - val_loss: 1.7370 - val_accuracy: 0.4333\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2264 - accuracy: 0.9889 - val_loss: 1.7386 - val_accuracy: 0.4333\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2231 - accuracy: 0.9889 - val_loss: 1.7424 - val_accuracy: 0.4333\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2202 - accuracy: 0.9889 - val_loss: 1.7445 - val_accuracy: 0.4333\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2176 - accuracy: 0.9889 - val_loss: 1.7417 - val_accuracy: 0.4333\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2147 - accuracy: 0.9889 - val_loss: 1.7407 - val_accuracy: 0.4333\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2126 - accuracy: 0.9889 - val_loss: 1.7352 - val_accuracy: 0.4333\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2093 - accuracy: 0.9926 - val_loss: 1.7355 - val_accuracy: 0.4333\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9926 - val_loss: 1.7393 - val_accuracy: 0.4222\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2046 - accuracy: 0.9889 - val_loss: 1.7367 - val_accuracy: 0.4333\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2016 - accuracy: 0.9926 - val_loss: 1.7440 - val_accuracy: 0.4333\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1993 - accuracy: 0.9889 - val_loss: 1.7485 - val_accuracy: 0.4333\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1966 - accuracy: 0.9926 - val_loss: 1.7471 - val_accuracy: 0.4333\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1941 - accuracy: 0.9926 - val_loss: 1.7482 - val_accuracy: 0.4333\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1920 - accuracy: 0.9926 - val_loss: 1.7473 - val_accuracy: 0.4222\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1901 - accuracy: 0.9926 - val_loss: 1.7499 - val_accuracy: 0.4333\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1873 - accuracy: 0.9926 - val_loss: 1.7468 - val_accuracy: 0.4333\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1849 - accuracy: 0.9926 - val_loss: 1.7498 - val_accuracy: 0.4222\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1830 - accuracy: 0.9926 - val_loss: 1.7551 - val_accuracy: 0.4222\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1805 - accuracy: 0.9926 - val_loss: 1.7527 - val_accuracy: 0.4333\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1786 - accuracy: 0.9926 - val_loss: 1.7464 - val_accuracy: 0.4222\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1764 - accuracy: 0.9926 - val_loss: 1.7492 - val_accuracy: 0.4222\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1746 - accuracy: 0.9926 - val_loss: 1.7574 - val_accuracy: 0.4222\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1725 - accuracy: 0.9926 - val_loss: 1.7583 - val_accuracy: 0.4222\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1704 - accuracy: 0.9926 - val_loss: 1.7533 - val_accuracy: 0.4222\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1684 - accuracy: 0.9926 - val_loss: 1.7551 - val_accuracy: 0.4222\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1665 - accuracy: 0.9926 - val_loss: 1.7579 - val_accuracy: 0.4222\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1645 - accuracy: 0.9963 - val_loss: 1.7581 - val_accuracy: 0.4222\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1627 - accuracy: 0.9926 - val_loss: 1.7582 - val_accuracy: 0.4222\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1608 - accuracy: 0.9926 - val_loss: 1.7634 - val_accuracy: 0.4222\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1590 - accuracy: 0.9963 - val_loss: 1.7617 - val_accuracy: 0.4222\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1575 - accuracy: 0.9963 - val_loss: 1.7630 - val_accuracy: 0.4222\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1557 - accuracy: 0.9963 - val_loss: 1.7618 - val_accuracy: 0.4222\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1537 - accuracy: 0.9963 - val_loss: 1.7629 - val_accuracy: 0.4222\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1522 - accuracy: 0.9963 - val_loss: 1.7607 - val_accuracy: 0.4222\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1506 - accuracy: 0.9963 - val_loss: 1.7722 - val_accuracy: 0.4222\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1488 - accuracy: 0.9963 - val_loss: 1.7681 - val_accuracy: 0.4222\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1471 - accuracy: 0.9963 - val_loss: 1.7660 - val_accuracy: 0.4222\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1457 - accuracy: 0.9963 - val_loss: 1.7691 - val_accuracy: 0.4222\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1439 - accuracy: 0.9963 - val_loss: 1.7683 - val_accuracy: 0.4222\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1424 - accuracy: 0.9963 - val_loss: 1.7708 - val_accuracy: 0.4222\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1411 - accuracy: 0.9963 - val_loss: 1.7692 - val_accuracy: 0.4222\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1397 - accuracy: 0.9963 - val_loss: 1.7665 - val_accuracy: 0.4222\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1381 - accuracy: 1.0000 - val_loss: 1.7696 - val_accuracy: 0.4222\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1365 - accuracy: 1.0000 - val_loss: 1.7692 - val_accuracy: 0.4222\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1354 - accuracy: 1.0000 - val_loss: 1.7737 - val_accuracy: 0.4222\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1338 - accuracy: 1.0000 - val_loss: 1.7691 - val_accuracy: 0.4222\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1322 - accuracy: 1.0000 - val_loss: 1.7727 - val_accuracy: 0.4222\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1310 - accuracy: 1.0000 - val_loss: 1.7813 - val_accuracy: 0.4222\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1295 - accuracy: 1.0000 - val_loss: 1.7803 - val_accuracy: 0.4222\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1281 - accuracy: 1.0000 - val_loss: 1.7779 - val_accuracy: 0.4222\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1270 - accuracy: 1.0000 - val_loss: 1.7823 - val_accuracy: 0.4222\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1256 - accuracy: 1.0000 - val_loss: 1.7832 - val_accuracy: 0.4222\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1244 - accuracy: 1.0000 - val_loss: 1.7819 - val_accuracy: 0.4222\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1233 - accuracy: 1.0000 - val_loss: 1.7828 - val_accuracy: 0.4222\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1219 - accuracy: 1.0000 - val_loss: 1.7843 - val_accuracy: 0.4222\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1209 - accuracy: 1.0000 - val_loss: 1.7894 - val_accuracy: 0.4222\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1193 - accuracy: 1.0000 - val_loss: 1.7844 - val_accuracy: 0.4222\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1182 - accuracy: 1.0000 - val_loss: 1.7829 - val_accuracy: 0.4222\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1169 - accuracy: 1.0000 - val_loss: 1.7889 - val_accuracy: 0.4222\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1157 - accuracy: 1.0000 - val_loss: 1.7936 - val_accuracy: 0.4222\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1147 - accuracy: 1.0000 - val_loss: 1.7989 - val_accuracy: 0.4222\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1135 - accuracy: 1.0000 - val_loss: 1.7960 - val_accuracy: 0.4222\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1123 - accuracy: 1.0000 - val_loss: 1.7949 - val_accuracy: 0.4222\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1111 - accuracy: 1.0000 - val_loss: 1.7925 - val_accuracy: 0.4222\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 1.7945 - val_accuracy: 0.4222\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1089 - accuracy: 1.0000 - val_loss: 1.7997 - val_accuracy: 0.4222\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1080 - accuracy: 1.0000 - val_loss: 1.8035 - val_accuracy: 0.4222\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1071 - accuracy: 1.0000 - val_loss: 1.8050 - val_accuracy: 0.4222\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1059 - accuracy: 1.0000 - val_loss: 1.8035 - val_accuracy: 0.4222\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1049 - accuracy: 1.0000 - val_loss: 1.8046 - val_accuracy: 0.4222\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1040 - accuracy: 1.0000 - val_loss: 1.7985 - val_accuracy: 0.4222\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1029 - accuracy: 1.0000 - val_loss: 1.8031 - val_accuracy: 0.4222\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 1.8052 - val_accuracy: 0.4222\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1009 - accuracy: 1.0000 - val_loss: 1.8043 - val_accuracy: 0.4222\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1000 - accuracy: 1.0000 - val_loss: 1.7977 - val_accuracy: 0.4222\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0991 - accuracy: 1.0000 - val_loss: 1.8026 - val_accuracy: 0.4222\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0980 - accuracy: 1.0000 - val_loss: 1.8053 - val_accuracy: 0.4222\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0971 - accuracy: 1.0000 - val_loss: 1.8095 - val_accuracy: 0.4222\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0962 - accuracy: 1.0000 - val_loss: 1.8084 - val_accuracy: 0.4222\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0953 - accuracy: 1.0000 - val_loss: 1.8109 - val_accuracy: 0.4222\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0945 - accuracy: 1.0000 - val_loss: 1.8149 - val_accuracy: 0.4222\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0936 - accuracy: 1.0000 - val_loss: 1.8176 - val_accuracy: 0.4222\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0928 - accuracy: 1.0000 - val_loss: 1.8121 - val_accuracy: 0.4222\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 1.8177 - val_accuracy: 0.4222\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0911 - accuracy: 1.0000 - val_loss: 1.8226 - val_accuracy: 0.4222\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0902 - accuracy: 1.0000 - val_loss: 1.8183 - val_accuracy: 0.4222\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 1.8182 - val_accuracy: 0.4222\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 1.8216 - val_accuracy: 0.4222\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0878 - accuracy: 1.0000 - val_loss: 1.8244 - val_accuracy: 0.4222\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0870 - accuracy: 1.0000 - val_loss: 1.8264 - val_accuracy: 0.4222\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0862 - accuracy: 1.0000 - val_loss: 1.8212 - val_accuracy: 0.4222\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0857 - accuracy: 1.0000 - val_loss: 1.8272 - val_accuracy: 0.4222\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0847 - accuracy: 1.0000 - val_loss: 1.8231 - val_accuracy: 0.4222\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0838 - accuracy: 1.0000 - val_loss: 1.8238 - val_accuracy: 0.4222\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0831 - accuracy: 1.0000 - val_loss: 1.8241 - val_accuracy: 0.4222\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0824 - accuracy: 1.0000 - val_loss: 1.8329 - val_accuracy: 0.4222\n",
      "Epoch 1/200\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 2.5479 - accuracy: 0.1148 - val_loss: 2.6850 - val_accuracy: 0.1556\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5023 - accuracy: 0.1185 - val_loss: 2.6361 - val_accuracy: 0.1556\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4608 - accuracy: 0.1259 - val_loss: 2.5901 - val_accuracy: 0.1556\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4203 - accuracy: 0.1259 - val_loss: 2.5475 - val_accuracy: 0.1556\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3839 - accuracy: 0.1333 - val_loss: 2.5075 - val_accuracy: 0.1556\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3481 - accuracy: 0.1370 - val_loss: 2.4712 - val_accuracy: 0.1556\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3171 - accuracy: 0.1481 - val_loss: 2.4347 - val_accuracy: 0.1556\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2849 - accuracy: 0.1519 - val_loss: 2.4014 - val_accuracy: 0.1556\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2559 - accuracy: 0.1593 - val_loss: 2.3695 - val_accuracy: 0.1556\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2268 - accuracy: 0.1741 - val_loss: 2.3401 - val_accuracy: 0.2000\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2012 - accuracy: 0.1815 - val_loss: 2.3120 - val_accuracy: 0.2000\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1766 - accuracy: 0.2000 - val_loss: 2.2862 - val_accuracy: 0.2111\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1530 - accuracy: 0.2148 - val_loss: 2.2621 - val_accuracy: 0.2556\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1315 - accuracy: 0.2222 - val_loss: 2.2378 - val_accuracy: 0.2556\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1104 - accuracy: 0.2296 - val_loss: 2.2148 - val_accuracy: 0.3000\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0895 - accuracy: 0.2407 - val_loss: 2.1939 - val_accuracy: 0.3111\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0712 - accuracy: 0.2370 - val_loss: 2.1747 - val_accuracy: 0.3111\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0530 - accuracy: 0.2481 - val_loss: 2.1563 - val_accuracy: 0.3111\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0359 - accuracy: 0.2667 - val_loss: 2.1388 - val_accuracy: 0.3000\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0199 - accuracy: 0.2704 - val_loss: 2.1224 - val_accuracy: 0.3000\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0039 - accuracy: 0.2778 - val_loss: 2.1076 - val_accuracy: 0.3000\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.9892 - accuracy: 0.2852 - val_loss: 2.0929 - val_accuracy: 0.3000\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.9747 - accuracy: 0.2926 - val_loss: 2.0785 - val_accuracy: 0.3000\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.9613 - accuracy: 0.2926 - val_loss: 2.0649 - val_accuracy: 0.3000\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.9479 - accuracy: 0.2926 - val_loss: 2.0523 - val_accuracy: 0.3000\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.9349 - accuracy: 0.3037 - val_loss: 2.0399 - val_accuracy: 0.3000\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.9225 - accuracy: 0.3037 - val_loss: 2.0281 - val_accuracy: 0.3000\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.9105 - accuracy: 0.3037 - val_loss: 2.0172 - val_accuracy: 0.3000\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.8989 - accuracy: 0.3037 - val_loss: 2.0073 - val_accuracy: 0.3000\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.8878 - accuracy: 0.3074 - val_loss: 1.9965 - val_accuracy: 0.3000\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.8762 - accuracy: 0.3074 - val_loss: 1.9866 - val_accuracy: 0.3111\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.8654 - accuracy: 0.3074 - val_loss: 1.9762 - val_accuracy: 0.3111\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 9ms/step - loss: 1.8545 - accuracy: 0.3111 - val_loss: 1.9662 - val_accuracy: 0.3111\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.8439 - accuracy: 0.3148 - val_loss: 1.9576 - val_accuracy: 0.3111\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.8342 - accuracy: 0.3148 - val_loss: 1.9482 - val_accuracy: 0.3111\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.8243 - accuracy: 0.3148 - val_loss: 1.9397 - val_accuracy: 0.3111\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.8145 - accuracy: 0.3111 - val_loss: 1.9312 - val_accuracy: 0.3111\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.8051 - accuracy: 0.3111 - val_loss: 1.9240 - val_accuracy: 0.3111\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7965 - accuracy: 0.3111 - val_loss: 1.9162 - val_accuracy: 0.3111\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7875 - accuracy: 0.3111 - val_loss: 1.9084 - val_accuracy: 0.3111\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.7786 - accuracy: 0.3148 - val_loss: 1.9012 - val_accuracy: 0.3111\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7701 - accuracy: 0.3148 - val_loss: 1.8942 - val_accuracy: 0.3000\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.7613 - accuracy: 0.3148 - val_loss: 1.8875 - val_accuracy: 0.3000\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7534 - accuracy: 0.3222 - val_loss: 1.8804 - val_accuracy: 0.3000\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7450 - accuracy: 0.3296 - val_loss: 1.8746 - val_accuracy: 0.3000\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.7373 - accuracy: 0.3296 - val_loss: 1.8685 - val_accuracy: 0.3000\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.7297 - accuracy: 0.3296 - val_loss: 1.8627 - val_accuracy: 0.3000\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7226 - accuracy: 0.3370 - val_loss: 1.8570 - val_accuracy: 0.3000\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7153 - accuracy: 0.3370 - val_loss: 1.8512 - val_accuracy: 0.3111\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.7083 - accuracy: 0.3407 - val_loss: 1.8463 - val_accuracy: 0.3111\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.7014 - accuracy: 0.3407 - val_loss: 1.8416 - val_accuracy: 0.3111\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6945 - accuracy: 0.3407 - val_loss: 1.8374 - val_accuracy: 0.3222\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.6880 - accuracy: 0.3444 - val_loss: 1.8327 - val_accuracy: 0.3222\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6815 - accuracy: 0.3481 - val_loss: 1.8279 - val_accuracy: 0.3222\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6750 - accuracy: 0.3519 - val_loss: 1.8244 - val_accuracy: 0.3222\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6688 - accuracy: 0.3481 - val_loss: 1.8207 - val_accuracy: 0.3222\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.6632 - accuracy: 0.3481 - val_loss: 1.8171 - val_accuracy: 0.3222\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6575 - accuracy: 0.3519 - val_loss: 1.8128 - val_accuracy: 0.3222\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6513 - accuracy: 0.3519 - val_loss: 1.8103 - val_accuracy: 0.3222\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.6455 - accuracy: 0.3556 - val_loss: 1.8073 - val_accuracy: 0.3222\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6400 - accuracy: 0.3556 - val_loss: 1.8049 - val_accuracy: 0.3222\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6349 - accuracy: 0.3556 - val_loss: 1.8021 - val_accuracy: 0.3222\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.6293 - accuracy: 0.3556 - val_loss: 1.7989 - val_accuracy: 0.3222\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6241 - accuracy: 0.3556 - val_loss: 1.7954 - val_accuracy: 0.3222\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6186 - accuracy: 0.3519 - val_loss: 1.7932 - val_accuracy: 0.3222\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6137 - accuracy: 0.3556 - val_loss: 1.7906 - val_accuracy: 0.3222\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.6084 - accuracy: 0.3630 - val_loss: 1.7877 - val_accuracy: 0.3222\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.6033 - accuracy: 0.3667 - val_loss: 1.7857 - val_accuracy: 0.3222\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5981 - accuracy: 0.3704 - val_loss: 1.7837 - val_accuracy: 0.3222\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5934 - accuracy: 0.3778 - val_loss: 1.7812 - val_accuracy: 0.3222\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.5882 - accuracy: 0.3778 - val_loss: 1.7788 - val_accuracy: 0.3222\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5837 - accuracy: 0.3815 - val_loss: 1.7772 - val_accuracy: 0.3444\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5790 - accuracy: 0.3815 - val_loss: 1.7752 - val_accuracy: 0.3444\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5743 - accuracy: 0.3815 - val_loss: 1.7737 - val_accuracy: 0.3444\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5696 - accuracy: 0.3815 - val_loss: 1.7716 - val_accuracy: 0.3444\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.5650 - accuracy: 0.3815 - val_loss: 1.7702 - val_accuracy: 0.3444\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5605 - accuracy: 0.3815 - val_loss: 1.7690 - val_accuracy: 0.3444\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5559 - accuracy: 0.3852 - val_loss: 1.7669 - val_accuracy: 0.3333\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5514 - accuracy: 0.3852 - val_loss: 1.7650 - val_accuracy: 0.3333\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5470 - accuracy: 0.3889 - val_loss: 1.7637 - val_accuracy: 0.3333\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5426 - accuracy: 0.3889 - val_loss: 1.7625 - val_accuracy: 0.3444\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.5379 - accuracy: 0.3852 - val_loss: 1.7610 - val_accuracy: 0.3444\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5337 - accuracy: 0.3852 - val_loss: 1.7600 - val_accuracy: 0.3556\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5293 - accuracy: 0.3852 - val_loss: 1.7586 - val_accuracy: 0.3556\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5249 - accuracy: 0.3852 - val_loss: 1.7574 - val_accuracy: 0.3556\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5206 - accuracy: 0.3852 - val_loss: 1.7564 - val_accuracy: 0.3556\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5162 - accuracy: 0.3852 - val_loss: 1.7552 - val_accuracy: 0.3556\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5122 - accuracy: 0.3852 - val_loss: 1.7537 - val_accuracy: 0.3556\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.5080 - accuracy: 0.3852 - val_loss: 1.7523 - val_accuracy: 0.3556\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.5037 - accuracy: 0.3852 - val_loss: 1.7511 - val_accuracy: 0.3556\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4994 - accuracy: 0.3889 - val_loss: 1.7503 - val_accuracy: 0.3556\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4953 - accuracy: 0.3889 - val_loss: 1.7498 - val_accuracy: 0.3556\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4914 - accuracy: 0.3889 - val_loss: 1.7482 - val_accuracy: 0.3444\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4870 - accuracy: 0.3926 - val_loss: 1.7474 - val_accuracy: 0.3444\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4830 - accuracy: 0.4000 - val_loss: 1.7472 - val_accuracy: 0.3444\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4790 - accuracy: 0.4037 - val_loss: 1.7466 - val_accuracy: 0.3556\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4749 - accuracy: 0.4074 - val_loss: 1.7452 - val_accuracy: 0.3556\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4711 - accuracy: 0.4074 - val_loss: 1.7440 - val_accuracy: 0.3556\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4670 - accuracy: 0.4074 - val_loss: 1.7431 - val_accuracy: 0.3556\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4632 - accuracy: 0.4074 - val_loss: 1.7419 - val_accuracy: 0.3556\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4593 - accuracy: 0.4074 - val_loss: 1.7410 - val_accuracy: 0.3556\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4553 - accuracy: 0.4111 - val_loss: 1.7404 - val_accuracy: 0.3556\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4516 - accuracy: 0.4111 - val_loss: 1.7395 - val_accuracy: 0.3556\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.4477 - accuracy: 0.4111 - val_loss: 1.7394 - val_accuracy: 0.3556\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4439 - accuracy: 0.4185 - val_loss: 1.7386 - val_accuracy: 0.3556\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4401 - accuracy: 0.4185 - val_loss: 1.7386 - val_accuracy: 0.3556\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.4362 - accuracy: 0.4222 - val_loss: 1.7375 - val_accuracy: 0.3556\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4324 - accuracy: 0.4222 - val_loss: 1.7365 - val_accuracy: 0.3556\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.4285 - accuracy: 0.4259 - val_loss: 1.7356 - val_accuracy: 0.3556\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4246 - accuracy: 0.4296 - val_loss: 1.7345 - val_accuracy: 0.3556\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.4208 - accuracy: 0.4296 - val_loss: 1.7341 - val_accuracy: 0.3556\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4170 - accuracy: 0.4296 - val_loss: 1.7337 - val_accuracy: 0.3556\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4132 - accuracy: 0.4296 - val_loss: 1.7326 - val_accuracy: 0.3556\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4094 - accuracy: 0.4333 - val_loss: 1.7322 - val_accuracy: 0.3556\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.4055 - accuracy: 0.4370 - val_loss: 1.7313 - val_accuracy: 0.3556\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.4020 - accuracy: 0.4370 - val_loss: 1.7301 - val_accuracy: 0.3556\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3981 - accuracy: 0.4370 - val_loss: 1.7297 - val_accuracy: 0.3556\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3948 - accuracy: 0.4407 - val_loss: 1.7288 - val_accuracy: 0.3556\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3906 - accuracy: 0.4444 - val_loss: 1.7280 - val_accuracy: 0.3556\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3872 - accuracy: 0.4481 - val_loss: 1.7273 - val_accuracy: 0.3556\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3836 - accuracy: 0.4519 - val_loss: 1.7266 - val_accuracy: 0.3556\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.3798 - accuracy: 0.4519 - val_loss: 1.7260 - val_accuracy: 0.3556\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3763 - accuracy: 0.4519 - val_loss: 1.7251 - val_accuracy: 0.3556\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3727 - accuracy: 0.4556 - val_loss: 1.7244 - val_accuracy: 0.3556\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3691 - accuracy: 0.4593 - val_loss: 1.7239 - val_accuracy: 0.3556\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3656 - accuracy: 0.4593 - val_loss: 1.7235 - val_accuracy: 0.3556\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3620 - accuracy: 0.4630 - val_loss: 1.7228 - val_accuracy: 0.3556\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3587 - accuracy: 0.4630 - val_loss: 1.7223 - val_accuracy: 0.3556\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3550 - accuracy: 0.4630 - val_loss: 1.7215 - val_accuracy: 0.3556\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3516 - accuracy: 0.4630 - val_loss: 1.7211 - val_accuracy: 0.3556\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.3481 - accuracy: 0.4667 - val_loss: 1.7209 - val_accuracy: 0.3667\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3445 - accuracy: 0.4741 - val_loss: 1.7203 - val_accuracy: 0.3667\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.3410 - accuracy: 0.4704 - val_loss: 1.7195 - val_accuracy: 0.3667\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.3378 - accuracy: 0.4704 - val_loss: 1.7192 - val_accuracy: 0.3667\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3341 - accuracy: 0.4741 - val_loss: 1.7184 - val_accuracy: 0.3667\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3307 - accuracy: 0.4741 - val_loss: 1.7177 - val_accuracy: 0.3667\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3270 - accuracy: 0.4741 - val_loss: 1.7168 - val_accuracy: 0.3667\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3236 - accuracy: 0.4778 - val_loss: 1.7160 - val_accuracy: 0.3667\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3202 - accuracy: 0.4852 - val_loss: 1.7156 - val_accuracy: 0.3667\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3168 - accuracy: 0.4852 - val_loss: 1.7150 - val_accuracy: 0.3667\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3135 - accuracy: 0.4889 - val_loss: 1.7145 - val_accuracy: 0.3667\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.3101 - accuracy: 0.4889 - val_loss: 1.7140 - val_accuracy: 0.3667\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3067 - accuracy: 0.4889 - val_loss: 1.7136 - val_accuracy: 0.3667\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3036 - accuracy: 0.4889 - val_loss: 1.7129 - val_accuracy: 0.3667\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.3001 - accuracy: 0.4889 - val_loss: 1.7121 - val_accuracy: 0.3667\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2966 - accuracy: 0.4889 - val_loss: 1.7119 - val_accuracy: 0.3667\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2934 - accuracy: 0.4889 - val_loss: 1.7120 - val_accuracy: 0.3667\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2901 - accuracy: 0.4889 - val_loss: 1.7113 - val_accuracy: 0.3667\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2869 - accuracy: 0.4889 - val_loss: 1.7109 - val_accuracy: 0.3667\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2835 - accuracy: 0.4889 - val_loss: 1.7099 - val_accuracy: 0.3667\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.2804 - accuracy: 0.4889 - val_loss: 1.7091 - val_accuracy: 0.3667\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2772 - accuracy: 0.4926 - val_loss: 1.7087 - val_accuracy: 0.3667\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.2739 - accuracy: 0.4963 - val_loss: 1.7080 - val_accuracy: 0.3667\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2707 - accuracy: 0.5000 - val_loss: 1.7073 - val_accuracy: 0.3667\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2675 - accuracy: 0.5000 - val_loss: 1.7070 - val_accuracy: 0.3667\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2643 - accuracy: 0.5037 - val_loss: 1.7064 - val_accuracy: 0.3667\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2612 - accuracy: 0.5037 - val_loss: 1.7058 - val_accuracy: 0.3667\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2580 - accuracy: 0.5111 - val_loss: 1.7053 - val_accuracy: 0.3667\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2550 - accuracy: 0.5111 - val_loss: 1.7049 - val_accuracy: 0.3667\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.2517 - accuracy: 0.5148 - val_loss: 1.7040 - val_accuracy: 0.3667\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.2486 - accuracy: 0.5148 - val_loss: 1.7035 - val_accuracy: 0.3667\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2456 - accuracy: 0.5148 - val_loss: 1.7029 - val_accuracy: 0.3778\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2424 - accuracy: 0.5148 - val_loss: 1.7019 - val_accuracy: 0.3778\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2395 - accuracy: 0.5222 - val_loss: 1.7016 - val_accuracy: 0.3778\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2363 - accuracy: 0.5259 - val_loss: 1.7012 - val_accuracy: 0.3778\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2332 - accuracy: 0.5259 - val_loss: 1.7007 - val_accuracy: 0.3778\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2301 - accuracy: 0.5259 - val_loss: 1.7000 - val_accuracy: 0.3778\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2270 - accuracy: 0.5296 - val_loss: 1.6993 - val_accuracy: 0.3778\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2239 - accuracy: 0.5296 - val_loss: 1.6989 - val_accuracy: 0.3778\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2211 - accuracy: 0.5296 - val_loss: 1.6982 - val_accuracy: 0.3778\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2179 - accuracy: 0.5333 - val_loss: 1.6977 - val_accuracy: 0.3778\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2153 - accuracy: 0.5333 - val_loss: 1.6976 - val_accuracy: 0.3778\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2119 - accuracy: 0.5370 - val_loss: 1.6973 - val_accuracy: 0.3778\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2090 - accuracy: 0.5444 - val_loss: 1.6976 - val_accuracy: 0.3778\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2059 - accuracy: 0.5481 - val_loss: 1.6967 - val_accuracy: 0.3778\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2030 - accuracy: 0.5444 - val_loss: 1.6964 - val_accuracy: 0.3778\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.2000 - accuracy: 0.5444 - val_loss: 1.6953 - val_accuracy: 0.3778\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1971 - accuracy: 0.5519 - val_loss: 1.6944 - val_accuracy: 0.3778\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1941 - accuracy: 0.5556 - val_loss: 1.6941 - val_accuracy: 0.3778\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1912 - accuracy: 0.5593 - val_loss: 1.6941 - val_accuracy: 0.3778\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1883 - accuracy: 0.5630 - val_loss: 1.6935 - val_accuracy: 0.3778\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1853 - accuracy: 0.5667 - val_loss: 1.6931 - val_accuracy: 0.3778\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1826 - accuracy: 0.5704 - val_loss: 1.6924 - val_accuracy: 0.3778\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1796 - accuracy: 0.5741 - val_loss: 1.6916 - val_accuracy: 0.3778\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1768 - accuracy: 0.5741 - val_loss: 1.6913 - val_accuracy: 0.3778\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1739 - accuracy: 0.5741 - val_loss: 1.6907 - val_accuracy: 0.3778\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1713 - accuracy: 0.5741 - val_loss: 1.6900 - val_accuracy: 0.3778\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1680 - accuracy: 0.5704 - val_loss: 1.6896 - val_accuracy: 0.3778\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1651 - accuracy: 0.5741 - val_loss: 1.6889 - val_accuracy: 0.3778\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1623 - accuracy: 0.5778 - val_loss: 1.6886 - val_accuracy: 0.3778\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1595 - accuracy: 0.5778 - val_loss: 1.6878 - val_accuracy: 0.3778\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1566 - accuracy: 0.5778 - val_loss: 1.6876 - val_accuracy: 0.3778\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1538 - accuracy: 0.5778 - val_loss: 1.6875 - val_accuracy: 0.3778\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1509 - accuracy: 0.5815 - val_loss: 1.6869 - val_accuracy: 0.3778\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1481 - accuracy: 0.5815 - val_loss: 1.6864 - val_accuracy: 0.3778\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.1453 - accuracy: 0.5852 - val_loss: 1.6855 - val_accuracy: 0.3778\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1425 - accuracy: 0.5852 - val_loss: 1.6848 - val_accuracy: 0.3778\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1397 - accuracy: 0.5852 - val_loss: 1.6852 - val_accuracy: 0.3778\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1369 - accuracy: 0.5852 - val_loss: 1.6849 - val_accuracy: 0.3778\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 1.1344 - accuracy: 0.5889 - val_loss: 1.6851 - val_accuracy: 0.3778\n",
      "Epoch 1/200\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 2.5629 - accuracy: 0.1000 - val_loss: 2.7038 - val_accuracy: 0.1111\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5587 - accuracy: 0.1000 - val_loss: 2.6992 - val_accuracy: 0.1111\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5546 - accuracy: 0.1000 - val_loss: 2.6945 - val_accuracy: 0.1111\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5505 - accuracy: 0.1000 - val_loss: 2.6899 - val_accuracy: 0.1111\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.5466 - accuracy: 0.1000 - val_loss: 2.6854 - val_accuracy: 0.1111\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5424 - accuracy: 0.1000 - val_loss: 2.6810 - val_accuracy: 0.1111\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5384 - accuracy: 0.1000 - val_loss: 2.6765 - val_accuracy: 0.1111\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.5344 - accuracy: 0.1000 - val_loss: 2.6720 - val_accuracy: 0.1111\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5303 - accuracy: 0.1000 - val_loss: 2.6675 - val_accuracy: 0.1111\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5265 - accuracy: 0.1037 - val_loss: 2.6632 - val_accuracy: 0.1111\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5226 - accuracy: 0.1074 - val_loss: 2.6588 - val_accuracy: 0.1111\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.5186 - accuracy: 0.1037 - val_loss: 2.6545 - val_accuracy: 0.1111\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5148 - accuracy: 0.1037 - val_loss: 2.6504 - val_accuracy: 0.1111\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.5111 - accuracy: 0.1037 - val_loss: 2.6461 - val_accuracy: 0.1111\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5074 - accuracy: 0.1037 - val_loss: 2.6419 - val_accuracy: 0.1111\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.5036 - accuracy: 0.1037 - val_loss: 2.6379 - val_accuracy: 0.1111\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.5000 - accuracy: 0.1037 - val_loss: 2.6337 - val_accuracy: 0.1111\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4963 - accuracy: 0.1037 - val_loss: 2.6297 - val_accuracy: 0.1111\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4926 - accuracy: 0.1037 - val_loss: 2.6256 - val_accuracy: 0.1111\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4890 - accuracy: 0.1037 - val_loss: 2.6215 - val_accuracy: 0.1111\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4853 - accuracy: 0.1074 - val_loss: 2.6174 - val_accuracy: 0.1111\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4816 - accuracy: 0.1074 - val_loss: 2.6134 - val_accuracy: 0.1111\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4781 - accuracy: 0.1074 - val_loss: 2.6093 - val_accuracy: 0.1111\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4745 - accuracy: 0.1074 - val_loss: 2.6054 - val_accuracy: 0.1111\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4709 - accuracy: 0.1074 - val_loss: 2.6016 - val_accuracy: 0.1111\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4675 - accuracy: 0.1074 - val_loss: 2.5977 - val_accuracy: 0.1111\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4640 - accuracy: 0.1074 - val_loss: 2.5939 - val_accuracy: 0.1111\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4607 - accuracy: 0.1074 - val_loss: 2.5899 - val_accuracy: 0.1111\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4572 - accuracy: 0.1074 - val_loss: 2.5861 - val_accuracy: 0.1111\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4538 - accuracy: 0.1074 - val_loss: 2.5823 - val_accuracy: 0.1111\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4504 - accuracy: 0.1074 - val_loss: 2.5786 - val_accuracy: 0.1111\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4471 - accuracy: 0.1111 - val_loss: 2.5749 - val_accuracy: 0.1111\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.4437 - accuracy: 0.1111 - val_loss: 2.5712 - val_accuracy: 0.1111\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4404 - accuracy: 0.1111 - val_loss: 2.5674 - val_accuracy: 0.1111\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4370 - accuracy: 0.1111 - val_loss: 2.5637 - val_accuracy: 0.1111\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4337 - accuracy: 0.1111 - val_loss: 2.5601 - val_accuracy: 0.1111\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4304 - accuracy: 0.1111 - val_loss: 2.5564 - val_accuracy: 0.1111\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4271 - accuracy: 0.1111 - val_loss: 2.5527 - val_accuracy: 0.1111\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4239 - accuracy: 0.1111 - val_loss: 2.5491 - val_accuracy: 0.1111\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4207 - accuracy: 0.1148 - val_loss: 2.5457 - val_accuracy: 0.1111\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.4176 - accuracy: 0.1148 - val_loss: 2.5423 - val_accuracy: 0.1111\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.4144 - accuracy: 0.1148 - val_loss: 2.5389 - val_accuracy: 0.1111\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4115 - accuracy: 0.1148 - val_loss: 2.5354 - val_accuracy: 0.1111\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.4083 - accuracy: 0.1148 - val_loss: 2.5321 - val_accuracy: 0.1111\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4053 - accuracy: 0.1148 - val_loss: 2.5287 - val_accuracy: 0.1111\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.4023 - accuracy: 0.1148 - val_loss: 2.5254 - val_accuracy: 0.1111\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3994 - accuracy: 0.1185 - val_loss: 2.5222 - val_accuracy: 0.1111\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3964 - accuracy: 0.1222 - val_loss: 2.5191 - val_accuracy: 0.1111\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3936 - accuracy: 0.1222 - val_loss: 2.5158 - val_accuracy: 0.1222\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3906 - accuracy: 0.1259 - val_loss: 2.5126 - val_accuracy: 0.1222\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3877 - accuracy: 0.1296 - val_loss: 2.5095 - val_accuracy: 0.1333\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3848 - accuracy: 0.1296 - val_loss: 2.5064 - val_accuracy: 0.1333\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3819 - accuracy: 0.1333 - val_loss: 2.5032 - val_accuracy: 0.1333\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.3792 - accuracy: 0.1333 - val_loss: 2.5000 - val_accuracy: 0.1333\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3763 - accuracy: 0.1333 - val_loss: 2.4971 - val_accuracy: 0.1333\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3736 - accuracy: 0.1333 - val_loss: 2.4941 - val_accuracy: 0.1333\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.3709 - accuracy: 0.1333 - val_loss: 2.4910 - val_accuracy: 0.1444\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3682 - accuracy: 0.1333 - val_loss: 2.4880 - val_accuracy: 0.1444\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.3652 - accuracy: 0.1333 - val_loss: 2.4852 - val_accuracy: 0.1444\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3626 - accuracy: 0.1333 - val_loss: 2.4822 - val_accuracy: 0.1444\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3601 - accuracy: 0.1333 - val_loss: 2.4791 - val_accuracy: 0.1444\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3573 - accuracy: 0.1333 - val_loss: 2.4763 - val_accuracy: 0.1556\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3547 - accuracy: 0.1333 - val_loss: 2.4735 - val_accuracy: 0.1667\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3520 - accuracy: 0.1333 - val_loss: 2.4707 - val_accuracy: 0.1667\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3494 - accuracy: 0.1407 - val_loss: 2.4678 - val_accuracy: 0.1667\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3468 - accuracy: 0.1407 - val_loss: 2.4651 - val_accuracy: 0.1667\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3443 - accuracy: 0.1407 - val_loss: 2.4623 - val_accuracy: 0.1667\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3417 - accuracy: 0.1444 - val_loss: 2.4596 - val_accuracy: 0.1778\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3391 - accuracy: 0.1444 - val_loss: 2.4571 - val_accuracy: 0.1778\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3368 - accuracy: 0.1444 - val_loss: 2.4543 - val_accuracy: 0.1778\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3342 - accuracy: 0.1481 - val_loss: 2.4517 - val_accuracy: 0.1778\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3318 - accuracy: 0.1519 - val_loss: 2.4491 - val_accuracy: 0.1778\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3293 - accuracy: 0.1519 - val_loss: 2.4466 - val_accuracy: 0.1778\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3269 - accuracy: 0.1556 - val_loss: 2.4441 - val_accuracy: 0.1778\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3244 - accuracy: 0.1593 - val_loss: 2.4415 - val_accuracy: 0.1778\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3221 - accuracy: 0.1593 - val_loss: 2.4390 - val_accuracy: 0.1778\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3197 - accuracy: 0.1593 - val_loss: 2.4364 - val_accuracy: 0.1778\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3173 - accuracy: 0.1593 - val_loss: 2.4340 - val_accuracy: 0.1778\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3149 - accuracy: 0.1630 - val_loss: 2.4316 - val_accuracy: 0.1778\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3126 - accuracy: 0.1630 - val_loss: 2.4291 - val_accuracy: 0.1778\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.3102 - accuracy: 0.1630 - val_loss: 2.4266 - val_accuracy: 0.1778\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.3079 - accuracy: 0.1630 - val_loss: 2.4241 - val_accuracy: 0.1778\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.3056 - accuracy: 0.1667 - val_loss: 2.4217 - val_accuracy: 0.1778\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.3033 - accuracy: 0.1704 - val_loss: 2.4192 - val_accuracy: 0.1778\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 2.3010 - accuracy: 0.1704 - val_loss: 2.4168 - val_accuracy: 0.1778\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.2987 - accuracy: 0.1704 - val_loss: 2.4145 - val_accuracy: 0.1778\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2965 - accuracy: 0.1741 - val_loss: 2.4123 - val_accuracy: 0.2000\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2944 - accuracy: 0.1778 - val_loss: 2.4100 - val_accuracy: 0.2111\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2921 - accuracy: 0.1778 - val_loss: 2.4077 - val_accuracy: 0.2111\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2899 - accuracy: 0.1778 - val_loss: 2.4054 - val_accuracy: 0.2111\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.2878 - accuracy: 0.1778 - val_loss: 2.4031 - val_accuracy: 0.2111\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.2856 - accuracy: 0.1815 - val_loss: 2.4009 - val_accuracy: 0.2222\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.2835 - accuracy: 0.1815 - val_loss: 2.3988 - val_accuracy: 0.2222\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.2814 - accuracy: 0.1815 - val_loss: 2.3966 - val_accuracy: 0.2333\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2793 - accuracy: 0.1815 - val_loss: 2.3945 - val_accuracy: 0.2333\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2771 - accuracy: 0.1852 - val_loss: 2.3923 - val_accuracy: 0.2333\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2750 - accuracy: 0.1852 - val_loss: 2.3901 - val_accuracy: 0.2333\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2729 - accuracy: 0.1889 - val_loss: 2.3879 - val_accuracy: 0.2333\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2709 - accuracy: 0.1889 - val_loss: 2.3858 - val_accuracy: 0.2333\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2688 - accuracy: 0.1889 - val_loss: 2.3837 - val_accuracy: 0.2333\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2668 - accuracy: 0.1889 - val_loss: 2.3815 - val_accuracy: 0.2333\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2647 - accuracy: 0.1889 - val_loss: 2.3795 - val_accuracy: 0.2333\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2627 - accuracy: 0.1963 - val_loss: 2.3774 - val_accuracy: 0.2333\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2607 - accuracy: 0.2037 - val_loss: 2.3754 - val_accuracy: 0.2333\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2586 - accuracy: 0.2037 - val_loss: 2.3734 - val_accuracy: 0.2333\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2567 - accuracy: 0.2074 - val_loss: 2.3713 - val_accuracy: 0.2444\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2547 - accuracy: 0.2111 - val_loss: 2.3694 - val_accuracy: 0.2444\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2528 - accuracy: 0.2148 - val_loss: 2.3673 - val_accuracy: 0.2444\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2508 - accuracy: 0.2148 - val_loss: 2.3654 - val_accuracy: 0.2556\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2489 - accuracy: 0.2148 - val_loss: 2.3636 - val_accuracy: 0.2556\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2471 - accuracy: 0.2148 - val_loss: 2.3616 - val_accuracy: 0.2556\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2452 - accuracy: 0.2148 - val_loss: 2.3597 - val_accuracy: 0.2556\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2433 - accuracy: 0.2148 - val_loss: 2.3579 - val_accuracy: 0.2556\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2415 - accuracy: 0.2222 - val_loss: 2.3561 - val_accuracy: 0.2556\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2397 - accuracy: 0.2222 - val_loss: 2.3542 - val_accuracy: 0.2556\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2378 - accuracy: 0.2222 - val_loss: 2.3523 - val_accuracy: 0.2556\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2359 - accuracy: 0.2222 - val_loss: 2.3506 - val_accuracy: 0.2556\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2341 - accuracy: 0.2296 - val_loss: 2.3487 - val_accuracy: 0.2556\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2323 - accuracy: 0.2296 - val_loss: 2.3469 - val_accuracy: 0.2556\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2305 - accuracy: 0.2296 - val_loss: 2.3451 - val_accuracy: 0.2556\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2287 - accuracy: 0.2296 - val_loss: 2.3433 - val_accuracy: 0.2556\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2269 - accuracy: 0.2259 - val_loss: 2.3415 - val_accuracy: 0.2556\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2251 - accuracy: 0.2296 - val_loss: 2.3398 - val_accuracy: 0.2556\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2233 - accuracy: 0.2296 - val_loss: 2.3379 - val_accuracy: 0.2556\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2215 - accuracy: 0.2296 - val_loss: 2.3363 - val_accuracy: 0.2556\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2198 - accuracy: 0.2296 - val_loss: 2.3345 - val_accuracy: 0.2556\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2181 - accuracy: 0.2296 - val_loss: 2.3328 - val_accuracy: 0.2556\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2164 - accuracy: 0.2296 - val_loss: 2.3311 - val_accuracy: 0.2556\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2147 - accuracy: 0.2296 - val_loss: 2.3293 - val_accuracy: 0.2556\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2130 - accuracy: 0.2296 - val_loss: 2.3278 - val_accuracy: 0.2556\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2113 - accuracy: 0.2296 - val_loss: 2.3261 - val_accuracy: 0.2556\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2097 - accuracy: 0.2296 - val_loss: 2.3246 - val_accuracy: 0.2556\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2080 - accuracy: 0.2296 - val_loss: 2.3229 - val_accuracy: 0.2444\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.2064 - accuracy: 0.2296 - val_loss: 2.3212 - val_accuracy: 0.2444\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2047 - accuracy: 0.2333 - val_loss: 2.3196 - val_accuracy: 0.2444\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2031 - accuracy: 0.2370 - val_loss: 2.3181 - val_accuracy: 0.2444\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.2014 - accuracy: 0.2407 - val_loss: 2.3165 - val_accuracy: 0.2444\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1999 - accuracy: 0.2407 - val_loss: 2.3149 - val_accuracy: 0.2444\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1983 - accuracy: 0.2407 - val_loss: 2.3134 - val_accuracy: 0.2444\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1967 - accuracy: 0.2407 - val_loss: 2.3118 - val_accuracy: 0.2444\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1951 - accuracy: 0.2481 - val_loss: 2.3104 - val_accuracy: 0.2444\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1935 - accuracy: 0.2481 - val_loss: 2.3088 - val_accuracy: 0.2444\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1919 - accuracy: 0.2519 - val_loss: 2.3073 - val_accuracy: 0.2556\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1904 - accuracy: 0.2519 - val_loss: 2.3058 - val_accuracy: 0.2667\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1888 - accuracy: 0.2519 - val_loss: 2.3043 - val_accuracy: 0.2778\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1873 - accuracy: 0.2519 - val_loss: 2.3029 - val_accuracy: 0.2778\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1858 - accuracy: 0.2519 - val_loss: 2.3015 - val_accuracy: 0.2778\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1843 - accuracy: 0.2519 - val_loss: 2.3000 - val_accuracy: 0.2889\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1828 - accuracy: 0.2519 - val_loss: 2.2986 - val_accuracy: 0.2889\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1813 - accuracy: 0.2519 - val_loss: 2.2972 - val_accuracy: 0.2889\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1798 - accuracy: 0.2519 - val_loss: 2.2958 - val_accuracy: 0.2778\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1783 - accuracy: 0.2519 - val_loss: 2.2944 - val_accuracy: 0.2778\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1768 - accuracy: 0.2519 - val_loss: 2.2929 - val_accuracy: 0.2778\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1753 - accuracy: 0.2556 - val_loss: 2.2915 - val_accuracy: 0.2778\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1738 - accuracy: 0.2556 - val_loss: 2.2902 - val_accuracy: 0.2778\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1724 - accuracy: 0.2556 - val_loss: 2.2888 - val_accuracy: 0.2778\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1710 - accuracy: 0.2556 - val_loss: 2.2875 - val_accuracy: 0.2778\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1695 - accuracy: 0.2556 - val_loss: 2.2861 - val_accuracy: 0.2778\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1680 - accuracy: 0.2556 - val_loss: 2.2846 - val_accuracy: 0.2889\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1665 - accuracy: 0.2556 - val_loss: 2.2832 - val_accuracy: 0.2889\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1651 - accuracy: 0.2556 - val_loss: 2.2819 - val_accuracy: 0.2889\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1637 - accuracy: 0.2593 - val_loss: 2.2806 - val_accuracy: 0.2889\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1623 - accuracy: 0.2630 - val_loss: 2.2792 - val_accuracy: 0.2889\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1608 - accuracy: 0.2630 - val_loss: 2.2779 - val_accuracy: 0.2889\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1595 - accuracy: 0.2667 - val_loss: 2.2766 - val_accuracy: 0.2889\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1581 - accuracy: 0.2667 - val_loss: 2.2753 - val_accuracy: 0.2889\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1567 - accuracy: 0.2667 - val_loss: 2.2740 - val_accuracy: 0.2889\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1554 - accuracy: 0.2667 - val_loss: 2.2728 - val_accuracy: 0.2889\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1540 - accuracy: 0.2667 - val_loss: 2.2715 - val_accuracy: 0.2889\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1526 - accuracy: 0.2667 - val_loss: 2.2702 - val_accuracy: 0.2889\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1512 - accuracy: 0.2704 - val_loss: 2.2689 - val_accuracy: 0.2889\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1498 - accuracy: 0.2704 - val_loss: 2.2676 - val_accuracy: 0.2889\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1485 - accuracy: 0.2704 - val_loss: 2.2663 - val_accuracy: 0.2889\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1471 - accuracy: 0.2704 - val_loss: 2.2650 - val_accuracy: 0.2889\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1458 - accuracy: 0.2704 - val_loss: 2.2638 - val_accuracy: 0.2889\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1444 - accuracy: 0.2704 - val_loss: 2.2626 - val_accuracy: 0.2889\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1431 - accuracy: 0.2704 - val_loss: 2.2615 - val_accuracy: 0.2889\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1418 - accuracy: 0.2704 - val_loss: 2.2603 - val_accuracy: 0.2889\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1405 - accuracy: 0.2704 - val_loss: 2.2590 - val_accuracy: 0.2889\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1392 - accuracy: 0.2741 - val_loss: 2.2578 - val_accuracy: 0.2889\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1379 - accuracy: 0.2741 - val_loss: 2.2567 - val_accuracy: 0.2889\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1366 - accuracy: 0.2741 - val_loss: 2.2555 - val_accuracy: 0.2889\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1354 - accuracy: 0.2741 - val_loss: 2.2544 - val_accuracy: 0.2889\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1341 - accuracy: 0.2741 - val_loss: 2.2533 - val_accuracy: 0.2889\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1329 - accuracy: 0.2741 - val_loss: 2.2522 - val_accuracy: 0.2889\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1316 - accuracy: 0.2741 - val_loss: 2.2511 - val_accuracy: 0.2889\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1303 - accuracy: 0.2741 - val_loss: 2.2500 - val_accuracy: 0.2889\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1291 - accuracy: 0.2741 - val_loss: 2.2489 - val_accuracy: 0.2889\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1278 - accuracy: 0.2741 - val_loss: 2.2478 - val_accuracy: 0.2889\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1266 - accuracy: 0.2741 - val_loss: 2.2467 - val_accuracy: 0.2889\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1253 - accuracy: 0.2741 - val_loss: 2.2456 - val_accuracy: 0.2889\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1241 - accuracy: 0.2741 - val_loss: 2.2444 - val_accuracy: 0.2889\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1229 - accuracy: 0.2741 - val_loss: 2.2433 - val_accuracy: 0.2889\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1216 - accuracy: 0.2741 - val_loss: 2.2422 - val_accuracy: 0.2889\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1204 - accuracy: 0.2741 - val_loss: 2.2412 - val_accuracy: 0.2889\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1192 - accuracy: 0.2741 - val_loss: 2.2401 - val_accuracy: 0.2889\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.1180 - accuracy: 0.2741 - val_loss: 2.2391 - val_accuracy: 0.2889\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1169 - accuracy: 0.2741 - val_loss: 2.2380 - val_accuracy: 0.2889\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.1156 - accuracy: 0.2741 - val_loss: 2.2371 - val_accuracy: 0.2889\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 2.1145 - accuracy: 0.2778 - val_loss: 2.2360 - val_accuracy: 0.2889\n",
      "Epoch 1/200\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 2.0686 - accuracy: 0.2185 - val_loss: 2.1814 - val_accuracy: 0.2222\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0683 - accuracy: 0.2185 - val_loss: 2.1810 - val_accuracy: 0.2222\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0679 - accuracy: 0.2185 - val_loss: 2.1806 - val_accuracy: 0.2222\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0676 - accuracy: 0.2185 - val_loss: 2.1803 - val_accuracy: 0.2222\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0672 - accuracy: 0.2185 - val_loss: 2.1799 - val_accuracy: 0.2222\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0669 - accuracy: 0.2185 - val_loss: 2.1795 - val_accuracy: 0.2222\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0665 - accuracy: 0.2185 - val_loss: 2.1791 - val_accuracy: 0.2222\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0662 - accuracy: 0.2185 - val_loss: 2.1788 - val_accuracy: 0.2222\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0659 - accuracy: 0.2185 - val_loss: 2.1784 - val_accuracy: 0.2222\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0655 - accuracy: 0.2185 - val_loss: 2.1780 - val_accuracy: 0.2222\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0652 - accuracy: 0.2185 - val_loss: 2.1777 - val_accuracy: 0.2222\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0649 - accuracy: 0.2185 - val_loss: 2.1773 - val_accuracy: 0.2222\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0645 - accuracy: 0.2185 - val_loss: 2.1769 - val_accuracy: 0.2222\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0642 - accuracy: 0.2185 - val_loss: 2.1766 - val_accuracy: 0.2222\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0639 - accuracy: 0.2185 - val_loss: 2.1762 - val_accuracy: 0.2222\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0635 - accuracy: 0.2185 - val_loss: 2.1758 - val_accuracy: 0.2222\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0632 - accuracy: 0.2185 - val_loss: 2.1755 - val_accuracy: 0.2222\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0628 - accuracy: 0.2185 - val_loss: 2.1751 - val_accuracy: 0.2222\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0625 - accuracy: 0.2185 - val_loss: 2.1747 - val_accuracy: 0.2111\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0622 - accuracy: 0.2185 - val_loss: 2.1744 - val_accuracy: 0.2111\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0618 - accuracy: 0.2185 - val_loss: 2.1740 - val_accuracy: 0.2111\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0615 - accuracy: 0.2185 - val_loss: 2.1736 - val_accuracy: 0.2111\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0612 - accuracy: 0.2185 - val_loss: 2.1733 - val_accuracy: 0.2111\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0608 - accuracy: 0.2185 - val_loss: 2.1729 - val_accuracy: 0.2111\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0605 - accuracy: 0.2185 - val_loss: 2.1726 - val_accuracy: 0.2111\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0602 - accuracy: 0.2185 - val_loss: 2.1722 - val_accuracy: 0.2111\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0598 - accuracy: 0.2185 - val_loss: 2.1718 - val_accuracy: 0.2111\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0595 - accuracy: 0.2185 - val_loss: 2.1715 - val_accuracy: 0.2111\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0592 - accuracy: 0.2185 - val_loss: 2.1711 - val_accuracy: 0.2111\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0588 - accuracy: 0.2185 - val_loss: 2.1708 - val_accuracy: 0.2111\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0585 - accuracy: 0.2185 - val_loss: 2.1704 - val_accuracy: 0.2111\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0582 - accuracy: 0.2185 - val_loss: 2.1700 - val_accuracy: 0.2222\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0578 - accuracy: 0.2185 - val_loss: 2.1697 - val_accuracy: 0.2222\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0575 - accuracy: 0.2185 - val_loss: 2.1693 - val_accuracy: 0.2222\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0572 - accuracy: 0.2185 - val_loss: 2.1689 - val_accuracy: 0.2222\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0568 - accuracy: 0.2185 - val_loss: 2.1686 - val_accuracy: 0.2222\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0565 - accuracy: 0.2185 - val_loss: 2.1682 - val_accuracy: 0.2222\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0562 - accuracy: 0.2185 - val_loss: 2.1679 - val_accuracy: 0.2222\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0559 - accuracy: 0.2185 - val_loss: 2.1675 - val_accuracy: 0.2222\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0556 - accuracy: 0.2185 - val_loss: 2.1672 - val_accuracy: 0.2222\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0552 - accuracy: 0.2185 - val_loss: 2.1668 - val_accuracy: 0.2222\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0549 - accuracy: 0.2185 - val_loss: 2.1664 - val_accuracy: 0.2222\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0546 - accuracy: 0.2185 - val_loss: 2.1661 - val_accuracy: 0.2222\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0542 - accuracy: 0.2185 - val_loss: 2.1657 - val_accuracy: 0.2222\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0539 - accuracy: 0.2185 - val_loss: 2.1653 - val_accuracy: 0.2222\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0536 - accuracy: 0.2185 - val_loss: 2.1650 - val_accuracy: 0.2222\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0532 - accuracy: 0.2222 - val_loss: 2.1646 - val_accuracy: 0.2222\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0529 - accuracy: 0.2222 - val_loss: 2.1643 - val_accuracy: 0.2222\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0526 - accuracy: 0.2222 - val_loss: 2.1639 - val_accuracy: 0.2222\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0523 - accuracy: 0.2222 - val_loss: 2.1635 - val_accuracy: 0.2222\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0519 - accuracy: 0.2222 - val_loss: 2.1632 - val_accuracy: 0.2222\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0516 - accuracy: 0.2222 - val_loss: 2.1628 - val_accuracy: 0.2222\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0513 - accuracy: 0.2222 - val_loss: 2.1625 - val_accuracy: 0.2222\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0510 - accuracy: 0.2222 - val_loss: 2.1621 - val_accuracy: 0.2222\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0506 - accuracy: 0.2222 - val_loss: 2.1617 - val_accuracy: 0.2222\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0503 - accuracy: 0.2222 - val_loss: 2.1614 - val_accuracy: 0.2222\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0500 - accuracy: 0.2222 - val_loss: 2.1610 - val_accuracy: 0.2222\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0497 - accuracy: 0.2222 - val_loss: 2.1607 - val_accuracy: 0.2222\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0493 - accuracy: 0.2222 - val_loss: 2.1603 - val_accuracy: 0.2222\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0490 - accuracy: 0.2222 - val_loss: 2.1600 - val_accuracy: 0.2222\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0487 - accuracy: 0.2222 - val_loss: 2.1596 - val_accuracy: 0.2222\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0484 - accuracy: 0.2222 - val_loss: 2.1592 - val_accuracy: 0.2333\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0480 - accuracy: 0.2222 - val_loss: 2.1589 - val_accuracy: 0.2333\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0477 - accuracy: 0.2222 - val_loss: 2.1586 - val_accuracy: 0.2333\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0474 - accuracy: 0.2222 - val_loss: 2.1582 - val_accuracy: 0.2333\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0471 - accuracy: 0.2222 - val_loss: 2.1579 - val_accuracy: 0.2333\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0468 - accuracy: 0.2222 - val_loss: 2.1575 - val_accuracy: 0.2333\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0465 - accuracy: 0.2222 - val_loss: 2.1572 - val_accuracy: 0.2333\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0461 - accuracy: 0.2222 - val_loss: 2.1568 - val_accuracy: 0.2333\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0458 - accuracy: 0.2222 - val_loss: 2.1565 - val_accuracy: 0.2333\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0455 - accuracy: 0.2222 - val_loss: 2.1561 - val_accuracy: 0.2333\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0452 - accuracy: 0.2222 - val_loss: 2.1558 - val_accuracy: 0.2333\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0449 - accuracy: 0.2222 - val_loss: 2.1554 - val_accuracy: 0.2333\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0446 - accuracy: 0.2222 - val_loss: 2.1551 - val_accuracy: 0.2333\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0442 - accuracy: 0.2222 - val_loss: 2.1547 - val_accuracy: 0.2333\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0439 - accuracy: 0.2222 - val_loss: 2.1544 - val_accuracy: 0.2333\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0436 - accuracy: 0.2222 - val_loss: 2.1540 - val_accuracy: 0.2444\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0433 - accuracy: 0.2185 - val_loss: 2.1537 - val_accuracy: 0.2444\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0430 - accuracy: 0.2222 - val_loss: 2.1534 - val_accuracy: 0.2444\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0427 - accuracy: 0.2222 - val_loss: 2.1530 - val_accuracy: 0.2444\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0424 - accuracy: 0.2222 - val_loss: 2.1527 - val_accuracy: 0.2444\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0420 - accuracy: 0.2222 - val_loss: 2.1523 - val_accuracy: 0.2444\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0417 - accuracy: 0.2222 - val_loss: 2.1520 - val_accuracy: 0.2444\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0414 - accuracy: 0.2222 - val_loss: 2.1516 - val_accuracy: 0.2444\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0411 - accuracy: 0.2222 - val_loss: 2.1513 - val_accuracy: 0.2444\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0408 - accuracy: 0.2222 - val_loss: 2.1509 - val_accuracy: 0.2444\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0405 - accuracy: 0.2222 - val_loss: 2.1506 - val_accuracy: 0.2444\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0401 - accuracy: 0.2222 - val_loss: 2.1502 - val_accuracy: 0.2444\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0398 - accuracy: 0.2222 - val_loss: 2.1499 - val_accuracy: 0.2444\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0395 - accuracy: 0.2222 - val_loss: 2.1495 - val_accuracy: 0.2444\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0392 - accuracy: 0.2222 - val_loss: 2.1492 - val_accuracy: 0.2556\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0389 - accuracy: 0.2222 - val_loss: 2.1488 - val_accuracy: 0.2556\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0386 - accuracy: 0.2222 - val_loss: 2.1485 - val_accuracy: 0.2556\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0382 - accuracy: 0.2222 - val_loss: 2.1482 - val_accuracy: 0.2556\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0379 - accuracy: 0.2222 - val_loss: 2.1478 - val_accuracy: 0.2556\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0376 - accuracy: 0.2259 - val_loss: 2.1475 - val_accuracy: 0.2556\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0373 - accuracy: 0.2259 - val_loss: 2.1471 - val_accuracy: 0.2556\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0370 - accuracy: 0.2259 - val_loss: 2.1468 - val_accuracy: 0.2556\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0367 - accuracy: 0.2259 - val_loss: 2.1464 - val_accuracy: 0.2556\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0364 - accuracy: 0.2259 - val_loss: 2.1461 - val_accuracy: 0.2556\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0361 - accuracy: 0.2259 - val_loss: 2.1457 - val_accuracy: 0.2556\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0358 - accuracy: 0.2259 - val_loss: 2.1454 - val_accuracy: 0.2556\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0354 - accuracy: 0.2259 - val_loss: 2.1451 - val_accuracy: 0.2556\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0351 - accuracy: 0.2259 - val_loss: 2.1447 - val_accuracy: 0.2667\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0348 - accuracy: 0.2259 - val_loss: 2.1444 - val_accuracy: 0.2667\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0345 - accuracy: 0.2259 - val_loss: 2.1440 - val_accuracy: 0.2667\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0342 - accuracy: 0.2259 - val_loss: 2.1437 - val_accuracy: 0.2667\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0339 - accuracy: 0.2259 - val_loss: 2.1434 - val_accuracy: 0.2667\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0336 - accuracy: 0.2259 - val_loss: 2.1430 - val_accuracy: 0.2667\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0333 - accuracy: 0.2259 - val_loss: 2.1427 - val_accuracy: 0.2667\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0330 - accuracy: 0.2259 - val_loss: 2.1424 - val_accuracy: 0.2667\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0327 - accuracy: 0.2259 - val_loss: 2.1420 - val_accuracy: 0.2667\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0324 - accuracy: 0.2259 - val_loss: 2.1417 - val_accuracy: 0.2667\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0321 - accuracy: 0.2259 - val_loss: 2.1413 - val_accuracy: 0.2667\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0318 - accuracy: 0.2259 - val_loss: 2.1410 - val_accuracy: 0.2667\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0315 - accuracy: 0.2259 - val_loss: 2.1407 - val_accuracy: 0.2667\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0312 - accuracy: 0.2259 - val_loss: 2.1404 - val_accuracy: 0.2667\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0309 - accuracy: 0.2259 - val_loss: 2.1400 - val_accuracy: 0.2667\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0306 - accuracy: 0.2259 - val_loss: 2.1397 - val_accuracy: 0.2667\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0303 - accuracy: 0.2296 - val_loss: 2.1394 - val_accuracy: 0.2667\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0300 - accuracy: 0.2296 - val_loss: 2.1391 - val_accuracy: 0.2667\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0297 - accuracy: 0.2296 - val_loss: 2.1387 - val_accuracy: 0.2667\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0294 - accuracy: 0.2296 - val_loss: 2.1384 - val_accuracy: 0.2667\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0291 - accuracy: 0.2296 - val_loss: 2.1381 - val_accuracy: 0.2778\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0288 - accuracy: 0.2296 - val_loss: 2.1377 - val_accuracy: 0.2778\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0285 - accuracy: 0.2296 - val_loss: 2.1374 - val_accuracy: 0.2778\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0282 - accuracy: 0.2296 - val_loss: 2.1371 - val_accuracy: 0.2778\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0279 - accuracy: 0.2296 - val_loss: 2.1367 - val_accuracy: 0.2778\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0276 - accuracy: 0.2296 - val_loss: 2.1364 - val_accuracy: 0.2778\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0273 - accuracy: 0.2296 - val_loss: 2.1361 - val_accuracy: 0.2778\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0270 - accuracy: 0.2296 - val_loss: 2.1357 - val_accuracy: 0.2778\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0267 - accuracy: 0.2296 - val_loss: 2.1354 - val_accuracy: 0.2778\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0264 - accuracy: 0.2296 - val_loss: 2.1350 - val_accuracy: 0.2778\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0261 - accuracy: 0.2296 - val_loss: 2.1347 - val_accuracy: 0.2778\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0258 - accuracy: 0.2296 - val_loss: 2.1344 - val_accuracy: 0.2778\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0255 - accuracy: 0.2296 - val_loss: 2.1341 - val_accuracy: 0.2778\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0252 - accuracy: 0.2296 - val_loss: 2.1337 - val_accuracy: 0.2778\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0249 - accuracy: 0.2296 - val_loss: 2.1334 - val_accuracy: 0.2778\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0246 - accuracy: 0.2296 - val_loss: 2.1331 - val_accuracy: 0.2778\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0243 - accuracy: 0.2296 - val_loss: 2.1328 - val_accuracy: 0.2778\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0240 - accuracy: 0.2296 - val_loss: 2.1324 - val_accuracy: 0.2778\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0237 - accuracy: 0.2296 - val_loss: 2.1321 - val_accuracy: 0.2778\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0234 - accuracy: 0.2296 - val_loss: 2.1318 - val_accuracy: 0.2778\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0231 - accuracy: 0.2296 - val_loss: 2.1314 - val_accuracy: 0.2778\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0228 - accuracy: 0.2296 - val_loss: 2.1311 - val_accuracy: 0.2889\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0225 - accuracy: 0.2296 - val_loss: 2.1308 - val_accuracy: 0.2889\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0222 - accuracy: 0.2296 - val_loss: 2.1305 - val_accuracy: 0.2889\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0220 - accuracy: 0.2296 - val_loss: 2.1302 - val_accuracy: 0.2889\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0217 - accuracy: 0.2296 - val_loss: 2.1298 - val_accuracy: 0.2889\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0214 - accuracy: 0.2296 - val_loss: 2.1295 - val_accuracy: 0.2889\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0211 - accuracy: 0.2296 - val_loss: 2.1292 - val_accuracy: 0.2889\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0208 - accuracy: 0.2296 - val_loss: 2.1289 - val_accuracy: 0.2889\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0205 - accuracy: 0.2296 - val_loss: 2.1286 - val_accuracy: 0.2889\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0202 - accuracy: 0.2296 - val_loss: 2.1282 - val_accuracy: 0.2889\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0199 - accuracy: 0.2296 - val_loss: 2.1279 - val_accuracy: 0.2889\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0196 - accuracy: 0.2296 - val_loss: 2.1276 - val_accuracy: 0.2889\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0193 - accuracy: 0.2296 - val_loss: 2.1273 - val_accuracy: 0.2889\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0190 - accuracy: 0.2333 - val_loss: 2.1269 - val_accuracy: 0.2889\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0188 - accuracy: 0.2333 - val_loss: 2.1266 - val_accuracy: 0.2889\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0185 - accuracy: 0.2333 - val_loss: 2.1263 - val_accuracy: 0.2889\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0182 - accuracy: 0.2333 - val_loss: 2.1260 - val_accuracy: 0.2889\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0179 - accuracy: 0.2333 - val_loss: 2.1257 - val_accuracy: 0.2889\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0176 - accuracy: 0.2333 - val_loss: 2.1254 - val_accuracy: 0.2889\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0173 - accuracy: 0.2333 - val_loss: 2.1250 - val_accuracy: 0.2889\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0170 - accuracy: 0.2333 - val_loss: 2.1247 - val_accuracy: 0.2889\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0167 - accuracy: 0.2333 - val_loss: 2.1244 - val_accuracy: 0.2889\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0165 - accuracy: 0.2333 - val_loss: 2.1241 - val_accuracy: 0.2889\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0162 - accuracy: 0.2333 - val_loss: 2.1238 - val_accuracy: 0.2889\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0159 - accuracy: 0.2333 - val_loss: 2.1234 - val_accuracy: 0.2889\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0156 - accuracy: 0.2333 - val_loss: 2.1231 - val_accuracy: 0.2889\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0153 - accuracy: 0.2333 - val_loss: 2.1228 - val_accuracy: 0.2889\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0150 - accuracy: 0.2333 - val_loss: 2.1224 - val_accuracy: 0.2889\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0147 - accuracy: 0.2333 - val_loss: 2.1221 - val_accuracy: 0.2889\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0144 - accuracy: 0.2333 - val_loss: 2.1218 - val_accuracy: 0.2889\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0141 - accuracy: 0.2333 - val_loss: 2.1215 - val_accuracy: 0.2889\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0138 - accuracy: 0.2333 - val_loss: 2.1212 - val_accuracy: 0.2889\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0136 - accuracy: 0.2333 - val_loss: 2.1209 - val_accuracy: 0.2889\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0133 - accuracy: 0.2333 - val_loss: 2.1205 - val_accuracy: 0.2889\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0130 - accuracy: 0.2333 - val_loss: 2.1202 - val_accuracy: 0.2889\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0127 - accuracy: 0.2333 - val_loss: 2.1199 - val_accuracy: 0.2889\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0124 - accuracy: 0.2333 - val_loss: 2.1196 - val_accuracy: 0.2889\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0121 - accuracy: 0.2333 - val_loss: 2.1193 - val_accuracy: 0.2889\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0118 - accuracy: 0.2370 - val_loss: 2.1189 - val_accuracy: 0.2889\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0116 - accuracy: 0.2370 - val_loss: 2.1186 - val_accuracy: 0.3000\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0113 - accuracy: 0.2370 - val_loss: 2.1183 - val_accuracy: 0.3000\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0110 - accuracy: 0.2370 - val_loss: 2.1180 - val_accuracy: 0.3000\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0107 - accuracy: 0.2370 - val_loss: 2.1177 - val_accuracy: 0.3000\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0104 - accuracy: 0.2370 - val_loss: 2.1174 - val_accuracy: 0.3000\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0101 - accuracy: 0.2370 - val_loss: 2.1171 - val_accuracy: 0.3000\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0099 - accuracy: 0.2370 - val_loss: 2.1168 - val_accuracy: 0.3000\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0096 - accuracy: 0.2370 - val_loss: 2.1165 - val_accuracy: 0.3000\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0093 - accuracy: 0.2370 - val_loss: 2.1162 - val_accuracy: 0.3000\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0090 - accuracy: 0.2370 - val_loss: 2.1158 - val_accuracy: 0.3000\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0088 - accuracy: 0.2370 - val_loss: 2.1155 - val_accuracy: 0.3000\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0085 - accuracy: 0.2370 - val_loss: 2.1152 - val_accuracy: 0.3000\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0082 - accuracy: 0.2370 - val_loss: 2.1149 - val_accuracy: 0.3000\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0079 - accuracy: 0.2370 - val_loss: 2.1146 - val_accuracy: 0.3000\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.0076 - accuracy: 0.2370 - val_loss: 2.1143 - val_accuracy: 0.3000\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 2.0074 - accuracy: 0.2370 - val_loss: 2.1140 - val_accuracy: 0.3000\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 2.0071 - accuracy: 0.2370 - val_loss: 2.1137 - val_accuracy: 0.3000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-------------------------------LEARNING CURVE FOR NEURAL NETWORK ONLY-----------------------------------------------#\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def fit_model(trainX, trainy, testX, testy, lrate):\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(120, input_dim=40000, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(6, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = Adam(lr=lrate)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\thistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "\t# plot learning curves\n",
    "\tpyplot.plot(history.history['accuracy'], label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], label='test')\n",
    "\tpyplot.title('lrate='+str(lrate), pad=-50)\n",
    "\n",
    "# create learning curves for different learning rates\n",
    "learning_rates = [1E-6, 1E-7,1E-8,1E-9]\n",
    "for i in range(len(learning_rates)):\n",
    "\t# determine the plot number\n",
    "\tplot_no = 420 + (i+1)\n",
    "\tpyplot.subplot(plot_no)\n",
    "\t# fit model and plot learning curves for a learning rate\n",
    "\tfit_model(X_train, y_train, X_test, y_test, learning_rates[i])\n",
    "# show learning curves\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4074846356453029 {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "#-------------------REGRESSION LOGISTIQUE WITH SCIKIT LEARN ---------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=4)\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "#score\n",
    "best_parameters = logreg_cv.best_params_\n",
    "best_accuracy = logreg_cv.best_score_\n",
    "print(best_accuracy,best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5592592592592592 {'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "#-------------------RANDOM FOREST WITH SCIKIT LEARN ---------------------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [50,100],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "#score\n",
    "best_parameters = CV_rfc.best_params_\n",
    "best_accuracy = CV_rfc.best_score_\n",
    "print(best_accuracy,best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5962962962962963 {'C': 1000.0, 'gamma': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "#-------------------SVM WITH SCIKIT LEARN ---------------------\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_grid = {'C': [1e3, 1e5],\n",
    "              'gamma': [0.0001, 0.1], }\n",
    "clf = GridSearchCV(\n",
    "    svm.SVC(kernel='rbf', class_weight='balanced'), param_grid\n",
    ")\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#score\n",
    "best_parameters = clf.best_params_\n",
    "best_accuracy = clf.best_score_\n",
    "print(best_accuracy,best_parameters)\n",
    "#scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# Use score method to get accuracy of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Neural Network WITH SCIKIT LEARN ---------------------\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "parameters = {'solver': ['lbfgs'], 'max_iter': [100,200 ], 'alpha': 10.0 ** -np.arange(1, 5), 'hidden_layer_sizes':np.arange(10, 12), 'random_state':[0,3,6,9]}\n",
    "ML = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "\n",
    "ML.fit(X_train, y_train)\n",
    "\n",
    "#score\n",
    "best_parameters = ML.best_params_\n",
    "best_accuracy = ML.best_score_\n",
    "print(best_accuracy,best_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPJ1qExlv6BQ"
   },
   "source": [
    "**<ins>Question</ins>: Évaluer les modèles appris en décrivant votre méthode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNAFZTFvv6BS"
   },
   "source": [
    "Pour chaques modèle on entraîne sur le jeu de donnée puis on test. \n",
    "Pour ce qui est des méthodes d'optimisation des hyper-paramètres, il y a deux méthodes:\n",
    "-Learning/validation curve \n",
    "elle permet de voir l'évolution de la qualité du modèle en fonction de l'entraînement. \n",
    "Très utile pour detecter le surentraînement et autre spécificités du modèle.\n",
    "\n",
    "-Grid search CV \n",
    "Permet de croiser tout les hyper-paramètres spécifiés et ressort le modèle avec les meilleurs hyper-paramètres.\n",
    "\n",
    "# Pour le réseau de neurones avec tensorflow:\n",
    "J'ai utilisé les deux méthodes \n",
    "Le réseau permet d'obtenir une accuracy de 46% sur les données d'entraînement et de 43% sur les données de test.\n",
    "\n",
    "# Regression logistique:\n",
    "j'utilise Grid search CV \n",
    "\n",
    "hyper-paramètres: Learning rate, C paramètre de régularisation\n",
    "\n",
    "J'obtiens un score de 41%\n",
    "\n",
    "# Random forest:\n",
    "j'utilise Grid search CV \n",
    "\n",
    "hyper-paramètres: Profondeur de l'arbre, nombre d'arbres dans la forêt \n",
    "\n",
    "J'obtiens un score de 35%\n",
    "\n",
    "# SVM:\n",
    "j'utilise Grid search CV \n",
    "\n",
    "hyper-paramètres: C paramètre de régularisation, gamma \n",
    "\n",
    "J'obtiens un score de 71%\n",
    "\n",
    "# Neural Network with Sklearn\n",
    "j'utilise Grid search CV \n",
    "\n",
    "hyper-paramètres: Learning rate, nombre d'hiden layer, le solver (algo backpropagation)...\n",
    "\n",
    "J'obtiens un score de 71%\n",
    "\n",
    "\n",
    "Le modèle SVM est donc le grand gagnant pour cette classification. \n",
    "Ce qui est prévisible car étant donné le jeu de données, c'est celui qui est recommandé par la SKlearn map que l'on peut retrouver sur l'API de la bibliothèque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Xw7bbYtv6BX"
   },
   "source": [
    "**<ins>Question</ins>: Réalisez un diagramme fonctionnel décrivant le flux des données tout au long de l'approche supervisée. Ce diagramme devra faire apparaître au minimum: les trois ensembles d'images, les descripteurs, les différents algorithmes d'apprentissage, l'évaluation (mettre une image dans le répertoire courant et dans la cellule ci-dessous remplacer par le nom du fichier)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDclLV6xv6BX"
   },
   "source": [
    " <img src=\"graphs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNgbxS-Fv6BY"
   },
   "source": [
    "# Partie 2: Approche supervisée sur descripteurs issus du scattering operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des descripteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYGGoa--v6BZ"
   },
   "source": [
    "**<ins>Question</ins>: Chargez les données du fichier matlab imdb_200x200_SmallSonarTex_db_6classes_scatValOnly.mat**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LmzGnm9v6BZ"
   },
   "source": [
    "_Your Code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0w4sdmcCv6Bb"
   },
   "outputs": [],
   "source": [
    "from pythonTools import *\n",
    "from usefulCmds import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Loading the csv file\n",
    "DATASET_PATH = r'./dataset/'\n",
    "DATASET_FILE = r'./imdb_200x200_SmallSonarTex_db_6classes_scatValOnly.mat'\n",
    "DATASET = os.path.join(DATASET_PATH, DATASET_FILE)\n",
    "\n",
    "DATASET_PATH = r'./dataset/imgs/'\n",
    "LABEL_PATH = r'./dataset/labels/labels.csv'\n",
    "dataset_df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "X = np.array([img.reshape(217,) for img in data_mat[\"featVal\"]])\n",
    "\n",
    "\n",
    "data_mat = loadmat(DATASET)\n",
    "\n",
    "#On récupère les labels\n",
    "label_names = dataset_df['seafloor']\n",
    "label_names_unique = label_names.unique()\n",
    "\n",
    "#  transformation des labels selon différents codages\n",
    "# indices\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(label_names_unique)\n",
    "label_indices = le.transform(label_names_unique)\n",
    "# Getting labels for our dataset\n",
    "y = le.transform(label_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpnR5ECxv6Bf"
   },
   "source": [
    "**<ins>Question</ins>: Y-a-t-il besoin de normaliser les descripteurs? Si oui, que faut-il conserver comme information et pourquoi?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1kxf9u-v6Bf"
   },
   "source": [
    "Oui il faut normaliser les descripteurs car cela permet de réduire le temps d'entraînement. Notamment pour les réseaux de neuronnes. En revanche , il faut garder la taille du vecteur et bien sûr les paramètres de la normalisation. Ces paramètres sont essentielles pour faire une prédiction. En effet, on entraîne le réseau sur un jeu de données normalisé. Il faut garder ces paramètres pour pouvoir présenter au réseau une image dans les même condition lors d'une prédiction ( moyenne , écart-type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LmzGnm9v6BZ"
   },
   "source": [
    "_Your Code below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 217) (270,) (90, 217) (90,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKfZD5Ymv6Bg"
   },
   "source": [
    "<strong><ins>Question</ins>: Séparer en deux ensembles de données et réalisez l'apprentissage successifs des modèles:\n",
    "* régression logistique, réseaux de neurones, svm et random forest en utilisant les fonctions du package scikit-learn\n",
    "</strong>\n",
    "\n",
    "<span style='color:red'> **Pas de code à développer ici, réutiliser celui de la partie 1**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixer les hyper paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01PQN1yev6Bl"
   },
   "source": [
    "**<ins>Question</ins>: Déterminez les hyper-paramètres (paramètre uniquement lié à l'algorithme d'apprentissage) de chaque algorithme. Comment allez vous les fixer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxHG8upGv6BO"
   },
   "source": [
    "Même façon que la partie 1. Lire la partie 1.\n",
    "\n",
    "Il y a deux possibilités de faire :\n",
    "\n",
    "Utiliser la learning/validation curve qui permet de visualiser l'erreur du réseau en fonction du nombre d'entraînement.\n",
    "\n",
    "Utiliser la fonction gridsearch cv bassée sur la cross validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPTVxU1Av6Bt"
   },
   "source": [
    "**<ins>Question</ins>:\n",
    "Lisez le [tutoriel suivant](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html\\#sphx-glr-auto-examples-applications-plot-face-recognition-py) en faisant particulièrement attention à la façon dont est gérée la détermination des hyperparamètres et l'évaluation des performances. Reproduisez cette méthodologie en testant différents nombres de plis (fold).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9888608428446005 {'C': 1.0, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "#-------------------REGRESSION LOGISTIQUE WITH SCIKIT LEARN ---------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=4)\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "#score\n",
    "best_parameters = logreg_cv.best_params_\n",
    "best_accuracy = logreg_cv.best_score_\n",
    "print(best_accuracy,best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9592592592592593 {'criterion': 'gini', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#-------------------RANDOM FOREST WITH SCIKIT LEARN ---------------------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [50,100],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "#score\n",
    "best_parameters = CV_rfc.best_params_\n",
    "best_accuracy = CV_rfc.best_score_\n",
    "print(best_accuracy,best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925925925925926 {'C': 1000.0, 'gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "#-------------------SVM WITH SCIKIT LEARN ---------------------\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_grid = {'C': [1e3,1e4, 1e5],\n",
    "              'gamma': [0.0001,0.001,0.005,0.01,0.05, 0.1], }\n",
    "clf = GridSearchCV(\n",
    "    svm.SVC(kernel='rbf', class_weight='balanced'), param_grid\n",
    ")\n",
    "clf = clf.fit(X_train, y_train)\n",
    "#score\n",
    "best_parameters = clf.best_params_\n",
    "best_accuracy = clf.best_score_\n",
    "print(best_accuracy,best_parameters)\n",
    "#scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# Use score method to get accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9962962962962962 {'alpha': 0.01, 'hidden_layer_sizes': 10, 'max_iter': 1000, 'random_state': 9, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "#-------------------Neural Network WITH SCIKIT LEARN ---------------------\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "parameters = {'solver': ['lbfgs'], 'max_iter': [1000,2000 ], 'alpha': 10.0 ** -np.arange(1, 5), 'hidden_layer_sizes':np.arange(10, 12), 'random_state':[0,3,6,9]}\n",
    "ML = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "\n",
    "ML.fit(X_train, y_train)\n",
    "\n",
    "#score\n",
    "best_parameters = ML.best_params_\n",
    "best_accuracy = ML.best_score_\n",
    "print(best_accuracy,best_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXG85EQLv6Bn"
   },
   "source": [
    "**<ins>Question</ins>: Évaluer les résultats et donner la valeur des paramètres optimaux**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2imQo5dcv6Bu"
   },
   "source": [
    "# Résultats\n",
    "\n",
    "## Regression logistique\n",
    "Best score = 0.9888608428446005\n",
    "\n",
    "Best parametres : C=1.0, 'penalty=l2\n",
    "\n",
    "## Random forest\n",
    "Best score = 0.9592592592592593\n",
    "\n",
    "Best parametres : criterion = gini, max_depth = 8, max_features = log2, n_estimators = 100\n",
    "\n",
    "## SVM\n",
    "Best score = 0.9925925925925926\n",
    "\n",
    "Best parametres : C = 1000.0, gamma = 0.01\n",
    "\n",
    "## Neural Network\n",
    "Best score = 0.9962962962962962\n",
    "\n",
    "Best parametres : alpha = 0.01, hidden_layer_sizes = 10, max_iter = 1000, random_state = 9, solver = lbfgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprendre le modèle final pour chaque classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       1.00      0.94      0.97        16\n",
      "           4       1.00      1.00      1.00        16\n",
      "           5       0.93      0.88      0.90        16\n",
      "\n",
      "    accuracy                           0.97        90\n",
      "   macro avg       0.96      0.97      0.96        90\n",
      "weighted avg       0.97      0.97      0.97        90\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[15  0  0  0  0  0]\n",
      " [ 0 17  0  0  0  0]\n",
      " [ 0  0 10  0  0  0]\n",
      " [ 0  0  0 15  0  1]\n",
      " [ 0  0  0  0 16  0]\n",
      " [ 0  0  2  0  0 14]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic regression\n",
    "logreg=LogisticRegression(C=1.0, penalty='l2')\n",
    "logreg.fit(X_train,y_train)\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = logreg.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (logreg, metrics.classification_report(y_test, predicted)))\n",
    "disp = metrics.plot_confusion_matrix(logreg, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=8, max_features='log2',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      0.94      0.97        17\n",
      "           2       0.69      0.90      0.78        10\n",
      "           3       0.94      0.94      0.94        16\n",
      "           4       1.00      1.00      1.00        16\n",
      "           5       0.93      0.81      0.87        16\n",
      "\n",
      "    accuracy                           0.93        90\n",
      "   macro avg       0.93      0.93      0.93        90\n",
      "weighted avg       0.94      0.93      0.94        90\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[15  0  0  0  0  0]\n",
      " [ 0 16  0  1  0  0]\n",
      " [ 0  0  9  0  0  1]\n",
      " [ 0  0  1 15  0  0]\n",
      " [ 0  0  0  0 16  0]\n",
      " [ 0  0  3  0  0 13]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "rfc=RandomForestClassifier(random_state=42,criterion='gini',max_depth=8, max_features='log2',n_estimators=100)\n",
    "rfc.fit(X_train,y_train)\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = rfc.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (rfc, metrics.classification_report(y_test, predicted)))\n",
    "disp = metrics.plot_confusion_matrix(rfc, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1000.0, break_ties=False, cache_size=200, class_weight='balanced',\n",
      "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.01,\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       0.91      1.00      0.95        10\n",
      "           3       1.00      0.94      0.97        16\n",
      "           4       1.00      1.00      1.00        16\n",
      "           5       0.94      0.94      0.94        16\n",
      "\n",
      "    accuracy                           0.98        90\n",
      "   macro avg       0.97      0.98      0.98        90\n",
      "weighted avg       0.98      0.98      0.98        90\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[15  0  0  0  0  0]\n",
      " [ 0 17  0  0  0  0]\n",
      " [ 0  0 10  0  0  0]\n",
      " [ 0  0  0 15  0  1]\n",
      " [ 0  0  0  0 16  0]\n",
      " [ 0  0  1  0  0 15]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "clf=svm.SVC(kernel='rbf', class_weight='balanced',C=1000.0, gamma= 0.01)\n",
    "clf.fit(X_train,y_train)\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (clf, metrics.classification_report(y_test, predicted)))\n",
    "disp = metrics.plot_confusion_matrix(clf, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=10, learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=9, shuffle=True, solver='lbfgs',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       0.91      1.00      0.95        10\n",
      "           3       1.00      0.94      0.97        16\n",
      "           4       1.00      1.00      1.00        16\n",
      "           5       0.94      0.94      0.94        16\n",
      "\n",
      "    accuracy                           0.98        90\n",
      "   macro avg       0.97      0.98      0.98        90\n",
      "weighted avg       0.98      0.98      0.98        90\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[15  0  0  0  0  0]\n",
      " [ 0 17  0  0  0  0]\n",
      " [ 0  0 10  0  0  0]\n",
      " [ 0  0  0 15  0  1]\n",
      " [ 0  0  0  0 16  0]\n",
      " [ 0  0  1  0  0 15]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Neural network\n",
    "ML=MLPClassifier(solver='lbfgs',alpha=0.01, hidden_layer_sizes=10, max_iter=1000, random_state=9)\n",
    "ML.fit(X_train,y_train)\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = ML.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (ML, metrics.classification_report(y_test, predicted)))\n",
    "disp = metrics.plot_confusion_matrix(ML, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rpSf449v6Bt"
   },
   "source": [
    "# Résultats des modèles finaux\n",
    "Dans cette dernière partie, on a entraîné les différents modèles avec les hyper-paramètres optimaux que nous avions déterminez dans la partie précédente. \n",
    "On a pu évaluer ces différents modèles sur le test set. \n",
    "On a pu visualiser notamment la matrice de confusion qui permet de visualiser les erreurs qu'à pu commettre le modèle lors de la prédiction des images du test set.\n",
    "\n",
    "## Regression logistique\n",
    "Comme on peut le visualiser sur la matrice de confusion le modèle de Regression logistique a fait 3 erreurs sur le test set.\n",
    "\n",
    "## Random forest\n",
    "Comme on peut le visualiser sur la matrice de confusion le modèle de Random forest a fait 6 erreurs sur le test set.\n",
    "\n",
    "## SVM\n",
    "Comme on peut le visualiser sur la matrice de confusion le modèle SVM a fait 2 erreurs sur le test set.\n",
    "\n",
    "## Neural Network\n",
    "Comme on peut le visualiser sur la matrice de confusion le modèle de Machine learning a fait 2 erreurs sur le test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnMpegufv6By"
   },
   "source": [
    "## Partie 4 Analyse finale des résultats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew9r6cPgv6By"
   },
   "source": [
    "**<ins>Question</ins>: Réalisez un diagramme fonctionnel décrivant le flux des données tout au long de l'approche supervisée. Ce diagramme devra faire apparaître au minimum: les trois ensembles d'images, les descripteurs, les différents algorithmes d'apprentissage, l'évaluation, les différents blocs de traitements.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k91uEbZGv6B1"
   },
   "source": [
    " <img src=\"graphs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVqpga1vv6B5"
   },
   "source": [
    "**<ins>Question</ins>: Faites une synthèse des résultats obtenus. Dresser en particulier des conclusions en fonction des descripteurs utilisés, des algorithmes utilisés et des prétraitements effectués.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUF82_f1v6B5"
   },
   "source": [
    "La synthèse des différents modèles a été réalisée précédement. On s'aperçoit que les modèles les plus performant sont le modèle de neural network et SVM qu'on pouvait prédire grace à la carte proposée par SKlearn.On remarque que le traitement de convolution et de pooling pour les données de la partie 2 a permis aux modèles de converger vers des résultats bien meilleurs qu'à la partie 1.\n",
    "Ainsi de façon général pour un traitement d'image on retiendra le modèle du neural network avec en amont un traitement de convolution pour l'extraction de features et pooling pour réduire la taille du vecteur d'entrée."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seafloorClassification_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
